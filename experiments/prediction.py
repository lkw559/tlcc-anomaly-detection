import json
from tqdm import tqdm
from eval_methods import *
from utils import *


class Predictor:
    """MTAD-GAT predictor class.

    :param model: MTAD-GAT model (pre-trained) used to forecast and reconstruct
    :param window_size: Length of the input sequence
    :param n_features: Number of input features
    :param pred_args: params for thresholding and predicting anomalies

    """

    def __init__(self, model, window_size, n_features, pred_args, summary_file_name="summary.txt"):
        self.model = model
        self.window_size = window_size
        self.n_features = n_features
        self.dataset = pred_args["dataset"]
        self.target_dims = pred_args["target_dims"]
        self.scale_scores = pred_args["scale_scores"]
        self.q = pred_args["q"]
        self.level = pred_args["level"]
        self.dynamic_pot = pred_args["dynamic_pot"]
        self.use_mov_av = pred_args["use_mov_av"]
        self.gamma = pred_args["gamma"]
        self.reg_level = pred_args["reg_level"]
        self.save_path = pred_args["save_path"]
        self.batch_size = 256
        self.use_cuda = True
        self.pred_args = pred_args
        self.summary_file_name = summary_file_name
        self.use_epsilon = pred_args.get("use_epsilon", True)
        self.use_pot = pred_args.get("use_pot", True)
        self.use_wadi_optimization = pred_args.get("use_wadi_optimization", False)

    def get_score(self, values):
        """Method that calculates anomaly score using given model and data
        :param values: 2D array of multivariate time series data, shape (N, k)
        :return np array of anomaly scores + dataframe with prediction for each channel and global anomalies
        """

        print("Predicting and calculating anomaly scores..")
        data = SlidingWindowDataset(values, self.window_size, self.target_dims)
        loader = torch.utils.data.DataLoader(data, batch_size=self.batch_size, shuffle=False)
        device = "cuda" if self.use_cuda and torch.cuda.is_available() else "cpu"

        self.model.eval()
        preds = []
        #recons = []
        with torch.no_grad():
            for x, y in tqdm(loader):
                x = x.to(device)
                y = y.to(device)

                #y_hat, _ = self.model(x)
                y_hat = self.model(x)

                # Shifting input to include the observed value (y) when doing the reconstruction
                #recon_x = torch.cat((x[:, 1:, :], y), dim=1)
                #_, window_recon = self.model(recon_x)

                preds.append(y_hat.detach().cpu().numpy())
                # Extract last reconstruction only
                #recons.append(window_recon[:, -1, :].detach().cpu().numpy())

        preds = np.concatenate(preds, axis=0)
        #recons = np.concatenate(recons, axis=0)
        actual = values.detach().cpu().numpy()[self.window_size:]

        if self.target_dims is not None:
            actual = actual[:, self.target_dims]

        anomaly_scores = np.zeros_like(actual)
        df = pd.DataFrame()
        for i in range(preds.shape[1]):
            df[f"Forecast_{i}"] = preds[:, i]
            #df[f"Recon_{i}"] = recons[:, i]
            df[f"True_{i}"] = actual[:, i]
            #a_score = np.sqrt((preds[:, i] - actual[:, i]) ** 2) + self.gamma * np.sqrt(
            #    (recons[:, i] - actual[:, i]) ** 2)  ########### Inculde Preds + Recon
            #a_score = np.sqrt((recons[:, i] - actual[:, i]) ** 2) ####### Remove Preds
            a_score = np.sqrt((preds[:, i] - actual[:, i]) ** 2) ####### Remove Recon

            if self.scale_scores:
                q75, q25 = np.percentile(a_score, [75, 25])
                iqr = q75 - q25
                median = np.median(a_score)
                a_score = (a_score - median) / (1+iqr)

            anomaly_scores[:, i] = a_score
            df[f"A_Score_{i}"] = a_score

        anomaly_scores = np.mean(anomaly_scores, 1)
        df['A_Score_Global'] = anomaly_scores

        return df

    def predict_anomalies(self, train, test, true_anomalies, load_scores=False, save_output=True,
                          scale_scores=False):
        """ Predicts anomalies

        :param train: 2D array of train multivariate time series data
        :param test: 2D array of test multivariate time series data
        :param true_anomalies: true anomalies of test set, None if not available
        :param save_scores: Whether to save anomaly scores of train and test
        :param load_scores: Whether to load anomaly scores instead of calculating them
        :param save_output: Whether to save output dataframe
        :param scale_scores: Whether to feature-wise scale anomaly scores
        """

        if load_scores:
            print("Loading anomaly scores")

            train_pred_df = pd.read_pickle(f"{self.save_path}/train_output.pkl")
            test_pred_df = pd.read_pickle(f"{self.save_path}/test_output.pkl")
            print('------------------------------------')
            print('train_pred_df.shape:',train_pred_df.shape)
            print('test_pred_df.shape:', test_pred_df.shape)

            train_anomaly_scores = train_pred_df['A_Score_Global'].values
            test_anomaly_scores = test_pred_df['A_Score_Global'].values

        else:
            train_pred_df = self.get_score(train)
            test_pred_df = self.get_score(test)


            train_anomaly_scores = train_pred_df['A_Score_Global'].values
            test_anomaly_scores = test_pred_df['A_Score_Global'].values

            train_anomaly_scores = adjust_anomaly_scores(train_anomaly_scores, self.dataset, True, self.window_size)
            test_anomaly_scores = adjust_anomaly_scores(test_anomaly_scores, self.dataset, False, self.window_size)

            # Update df
            train_pred_df['A_Score_Global'] = train_anomaly_scores
            test_pred_df['A_Score_Global'] = test_anomaly_scores

        if self.use_mov_av:
            smoothing_window = int(self.batch_size * self.window_size * 0.05)
            train_anomaly_scores = pd.DataFrame(train_anomaly_scores).ewm(span=smoothing_window).mean().values.flatten()
            test_anomaly_scores = pd.DataFrame(test_anomaly_scores).ewm(span=smoothing_window).mean().values.flatten()
        print('------------------------------------')
        print('train_pred_df.shape:',train_pred_df.shape)
        print('test_pred_df.shape:', test_pred_df.shape)
        print('------------------------------------')
        print('train_anomaly_scores:', len(train_anomaly_scores))
        print('test_anomaly_scores:', len(test_anomaly_scores))
        

        # Find threshold and predict anomalies at feature-level (for plotting and diagnosis purposes)
        out_dim = self.n_features if self.target_dims is None else len(self.target_dims)
        all_preds = np.zeros((len(test_pred_df), out_dim))
        for i in range(out_dim):
            train_feature_anom_scores = train_pred_df[f"A_Score_{i}"].values
            test_feature_anom_scores = test_pred_df[f"A_Score_{i}"].values
            epsilon = find_epsilon(train_feature_anom_scores, reg_level=2)

            train_feature_anom_preds = (train_feature_anom_scores >= epsilon).astype(int)
            test_feature_anom_preds = (test_feature_anom_scores >= epsilon).astype(int)

            train_pred_df[f"A_Pred_{i}"] = train_feature_anom_preds
            test_pred_df[f"A_Pred_{i}"] = test_feature_anom_preds

            train_pred_df[f"Thresh_{i}"] = epsilon
            test_pred_df[f"Thresh_{i}"] = epsilon

            all_preds[:, i] = test_feature_anom_preds

        # Global anomaly í‰ê°€ (Epsilon, POT ëª¨ë‘ ì§€ì›)
        summary = {}
        threshold_used = None
        
        # WADI ìµœì í™”ë¥¼ ìœ„í•œ train-validation split (ì—°êµ¬ìœ¤ë¦¬ ì¤€ìˆ˜)
        validation_scores = None
        validation_labels = None
        original_train_scores = train_anomaly_scores.copy()  # ì›ë³¸ ë³´ì¡´
        
        if self.use_wadi_optimization and true_anomalies is not None:
            # í›ˆë ¨ ë°ì´í„°ì˜ 80%ë¥¼ ì‹¤ì œ í›ˆë ¨ìš©, 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• 
            split_idx = int(len(train_anomaly_scores) * 0.8)
            actual_train_scores = train_anomaly_scores[:split_idx]
            validation_scores = train_anomaly_scores[split_idx:]
            
            # ê²€ì¦ ë¼ë²¨ ìƒì„± (ì •ìƒ ë°ì´í„°ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ê°€ì •)
            validation_labels = np.zeros(len(validation_scores))
            
            print(f"ðŸ”„ ì—°êµ¬ìœ¤ë¦¬ ì¤€ìˆ˜: Train-Validation Split ì ìš©")
            print(f"   Train: {len(actual_train_scores)} samples")
            print(f"   Validation: {len(validation_scores)} samples")
            
            # ìž„ê³„ê°’ ê³„ì‚°ìš©ìœ¼ë¡œë§Œ ë¶„í• ëœ ë°ì´í„° ì‚¬ìš©
            train_for_threshold = actual_train_scores
        else:
            train_for_threshold = train_anomaly_scores
        
        # Epsilon ë°©ì‹
        if self.use_epsilon:
            e_eval = epsilon_eval(train_for_threshold, test_anomaly_scores, true_anomalies, 
                                use_wadi_optimization=self.use_wadi_optimization,
                                validation_scores=validation_scores,
                                validation_labels=validation_labels)
            optimization_status = "WADI optimized (CV-based)" if self.use_wadi_optimization else "standard"
            print(f"Results using epsilon method ({optimization_status}):\n {e_eval}")
            
            # í™•ìž¥ëœ ì§€í‘œ ê³„ì‚° ë° ì¶œë ¥
            if true_anomalies is not None:
                from eval_methods import calculate_additional_metrics, print_extended_metrics, epsilon_eval_extended
                
                print("\nðŸš€ ì¶”ê°€ í‰ê°€ì§€í‘œ ê³„ì‚° ì¤‘...")
                extended_result = epsilon_eval_extended(
                    train_for_threshold, test_anomaly_scores, true_anomalies,
                    reg_level=1, use_wadi_optimization=self.use_wadi_optimization,
                    validation_scores=validation_scores, validation_labels=validation_labels
                )
                
                # í™•ìž¥ëœ ì§€í‘œ ì¶œë ¥
                print_extended_metrics(extended_result, f"Epsilon ({optimization_status})")
                
                # summaryì— í™•ìž¥ëœ ì§€í‘œë„ ì¶”ê°€
                summary["epsilon_extended"] = {k: float(v) if not isinstance(v, list) else v 
                                             for k, v in extended_result.items()}
            
            for k, v in e_eval.items():
                if not type(e_eval[k]) == list:
                    e_eval[k] = float(v)
            summary["epsilon_result"] = e_eval
            threshold_used = e_eval["threshold"]
        # POT ë°©ì‹
        if self.use_pot:
            if self.use_wadi_optimization:
                # WADI ìµœì í™”ëœ POT íŒŒë¼ë¯¸í„° ì‚¬ìš©
                pot_q = 0.05  # WADIì— ìµœì í™”ëœ q ê°’
                optimization_status = "WADI optimized"
            else:
                pot_q = self.q
                optimization_status = "standard"
                
            pot_eval_result = pot_eval(
                train_for_threshold,  # ìˆ˜ì •: ë¶„í• ëœ ë°ì´í„° ì‚¬ìš©
                test_anomaly_scores,
                true_anomalies,
                q=pot_q,
                level=self.level,
                dynamic=self.dynamic_pot
            )
            print(f"Results using POT method ({optimization_status}):\n {pot_eval_result}")
            
            # POT í™•ìž¥ëœ ì§€í‘œ ê³„ì‚° ë° ì¶œë ¥
            if true_anomalies is not None:
                from eval_methods import pot_eval_extended, print_extended_metrics
                
                print("\nðŸš€ POT ì¶”ê°€ í‰ê°€ì§€í‘œ ê³„ì‚° ì¤‘...")
                pot_extended_result = pot_eval_extended(
                    train_for_threshold, test_anomaly_scores, true_anomalies,
                    q=pot_q, level=self.level, dynamic=self.dynamic_pot
                )
                
                # í™•ìž¥ëœ ì§€í‘œ ì¶œë ¥
                print_extended_metrics(pot_extended_result, f"POT ({optimization_status})")
                
                # summaryì— í™•ìž¥ëœ ì§€í‘œë„ ì¶”ê°€
                summary["pot_extended"] = {k: float(v) if not isinstance(v, list) else v 
                                         for k, v in pot_extended_result.items()}
            
            for k, v in pot_eval_result.items():
                if not type(pot_eval_result[k]) == list:
                    pot_eval_result[k] = float(v)
            summary["pot_result"] = pot_eval_result
            if not self.use_epsilon:
                threshold_used = pot_eval_result["threshold"]
            elif threshold_used is None:
                threshold_used = pot_eval_result["threshold"]
        
        # ì¢…í•© ì„±ëŠ¥ í‰ê°€ ë° ê°€ì´ë“œ ì¶œë ¥ (ë…¸íŠ¸ë¶ ì‹¤í–‰ ì‹œ)
        if true_anomalies is not None and (self.use_epsilon or self.use_pot):
            print("\n" + "="*80)
            print("ðŸŽ¯ ì¢…í•© ì„±ëŠ¥ í‰ê°€ ë° ê°€ì´ë“œ")
            print("="*80)
            
            # ì‚¬ìš©ëœ ë°©ë²• í‘œì‹œ
            methods_used = []
            if self.use_epsilon:
                methods_used.append("Epsilon")
            if self.use_pot:
                methods_used.append("POT")
            print(f"ðŸ“Š ì ìš©ëœ í‰ê°€ ë°©ë²•: {', '.join(methods_used)}")
            
            # ìµœê³  ì„±ëŠ¥ ë°©ë²• ì‹ë³„
            best_method = None
            best_f1 = 0
            
            if "epsilon_extended" in summary:
                eps_f1 = summary["epsilon_extended"].get("f1", 0)
                if eps_f1 > best_f1:
                    best_f1 = eps_f1
                    best_method = "Epsilon"
            
            if "pot_extended" in summary:
                pot_f1 = summary["pot_extended"].get("f1", 0)
                if pot_f1 > best_f1:
                    best_f1 = pot_f1
                    best_method = "POT"
            
            if best_method:
                print(f"ðŸ† ìµœê³  ì„±ëŠ¥ ë°©ë²•: {best_method} (F1: {best_f1:.4f})")
                best_result = summary.get(f"{best_method.lower()}_extended", {})
                
                # ì„±ëŠ¥ ê°€ì´ë“œ (ë” ì •í™•í•œ ê¸°ì¤€ ì ìš©)
                print(f"\nðŸ“ˆ ì„±ëŠ¥ ì§€í‘œ í‰ê°€:")
                roc_auc = best_result.get("roc_auc", 0)
                pr_auc = best_result.get("pr_auc", 0) 
                mcc = best_result.get("mcc", 0)
                
                # ROC-AUC í‰ê°€ (ì´ìƒíƒì§€ íŠ¹í™” ê¸°ì¤€)
                if roc_auc >= 0.9:
                    print(f"   ROC-AUC {roc_auc:.4f}: íƒì›” ðŸŸ¢")
                elif roc_auc >= 0.8:
                    print(f"   ROC-AUC {roc_auc:.4f}: ìš°ìˆ˜ ðŸŸ¢")
                elif roc_auc >= 0.7:
                    print(f"   ROC-AUC {roc_auc:.4f}: ì–‘í˜¸ ðŸŸ¡")
                elif roc_auc >= 0.6:
                    print(f"   ROC-AUC {roc_auc:.4f}: ë³´í†µ ðŸŸ ")
                else:
                    print(f"   ROC-AUC {roc_auc:.4f}: ê°œì„ í•„ìš” ðŸ”´")
                
                # PR-AUC í‰ê°€ (ë¶ˆê· í˜• ë°ì´í„° íŠ¹í™” ê¸°ì¤€)
                if pr_auc >= 0.7:
                    print(f"   PR-AUC {pr_auc:.4f}: íƒì›” ðŸŸ¢")
                elif pr_auc >= 0.5:
                    print(f"   PR-AUC {pr_auc:.4f}: ìš°ìˆ˜ ðŸŸ¢")
                elif pr_auc >= 0.3:
                    print(f"   PR-AUC {pr_auc:.4f}: ì–‘í˜¸ ðŸŸ¡")
                elif pr_auc >= 0.2:
                    print(f"   PR-AUC {pr_auc:.4f}: ë³´í†µ ðŸŸ ")
                else:
                    print(f"   PR-AUC {pr_auc:.4f}: ê°œì„ í•„ìš” ðŸ”´")
                
                # MCC í‰ê°€ (ê· í˜• ì§€í‘œ)
                if mcc >= 0.8:
                    print(f"   MCC {mcc:.4f}: íƒì›” ðŸŸ¢")
                elif mcc >= 0.6:
                    print(f"   MCC {mcc:.4f}: ìš°ìˆ˜ ðŸŸ¢")
                elif mcc >= 0.4:
                    print(f"   MCC {mcc:.4f}: ì–‘í˜¸ ðŸŸ¡")
                elif mcc >= 0.2:
                    print(f"   MCC {mcc:.4f}: ë³´í†µ ðŸŸ ")
                else:
                    print(f"   MCC {mcc:.4f}: ê°œì„ í•„ìš” ðŸ”´")
            
            print("\nðŸ’¡ ì‹¤í—˜ ì°¸ê³ ì‚¬í•­:")
            if self.use_wadi_optimization:
                print("   âœ… ì—°êµ¬ìœ¤ë¦¬ ì¤€ìˆ˜: CV ê¸°ë°˜ ìž„ê³„ê°’ ì„ íƒ ì ìš©ë¨")
            print("   ðŸ“‹ ëª¨ë“  í™•ìž¥ ì§€í‘œê°€ summary íŒŒì¼ì— ì €ìž¥ë¨")
            print("   ðŸ”„ ë‹¤ë¥¸ TLCC threshold ê°’ ì‹¤í—˜ ê¶Œìž¥")
            print("="*80)
        
        # ê²°ê³¼ ì €ìž¥
        with open(f"{self.save_path}/{self.summary_file_name}", "w") as f:
            json.dump(summary, f, indent=2)

        # Save anomaly predictions made using epsilon method
        if save_output:
            global_epsilon = threshold_used
            test_pred_df["A_True_Global"] = true_anomalies
            train_pred_df["Thresh_Global"] = global_epsilon
            test_pred_df["Thresh_Global"] = global_epsilon
            train_pred_df[f"A_Pred_Global"] = (original_train_scores >= global_epsilon).astype(int)
            test_preds_global = (test_anomaly_scores >= global_epsilon).astype(int)
            # Adjust predictions according to evaluation strategy
            if true_anomalies is not None:
                test_preds_global = adjust_predicts(None, true_anomalies, global_epsilon, pred=test_preds_global)
            test_pred_df[f"A_Pred_Global"] = test_preds_global

            print(f"Saving output to {self.save_path}/<train/test>_output.pkl")
            train_pred_df.to_pickle(f"{self.save_path}/train_output.pkl")
            test_pred_df.to_pickle(f"{self.save_path}/test_output.pkl")

        print("-- Done.")

