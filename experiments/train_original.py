import json
import json
from datetime import datetime
import torch.nn as nn
import pickle
import numpy as np

from args import get_parser
from utils import *
from mtad_gat import MTAD_GAT
from prediction import Predictor
from training import Trainer

# cognite.correlation ëŒ€ì²´ êµ¬í˜„ ì‚¬ìš©
from correlation_alternative import columns_by_max_cross_correlation, plot_cross_correlations, cognite_mock as cognite

from datetime import datetime, timedelta
import seaborn as sns
import time

import random
import numpy as np
import torch
import torch.backends.cudnn as cudnn

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

########### warning hide ##############
import warnings
warnings.filterwarnings(action='ignore')
#######################################

start = time.time()
id = datetime.now().strftime("%d%m%Y_%H%M%S")
torch.cuda.empty_cache()


if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()

    dataset = args.dataset
    window_size = args.lookback
    spec_res = args.spec_res
    normalize = args.normalize
    n_epochs = args.epochs
    batch_size = args.bs
    init_lr = args.init_lr
    val_split = args.val_split
    shuffle_dataset = args.shuffle_dataset
    use_cuda = args.use_cuda
    print_every = args.print_every
    log_tensorboard = args.log_tensorboard
    group_index = args.group[0]
    index = args.group[2:]
    args_summary = str(args.__dict__)

    if dataset == "SMD":
        # SMD ì²˜ë¦¬
        output_path = f'output/SMD/{args.group}'
        (x_train, _), (x_test, y_test) = get_data(f"machine-{group_index}-{index}", normalize=normalize)
    elif dataset == "SMAP":
        # SMAP ì²˜ë¦¬
        output_path = f'output/{dataset}'
        (x_train, _), (x_test, y_test) = get_data(dataset, normalize=normalize)
    elif dataset == "MSL":
        # MSL ì²˜ë¦¬
        output_path = f'output/{dataset}'
        (x_train, _), (x_test, y_test) = get_data(dataset, normalize=normalize)
    elif dataset == "WADI":
        # WADI ì²˜ë¦¬
        output_path = f'output/WADI'
        (x_train, _), (x_test, y_test) = get_data('WADI', normalize=normalize)
        
        # Check for NaN/Inf in data and clean if necessary
        if np.isnan(x_train).any() or np.isinf(x_train).any():
            print("WARNING: NaN or Inf values found in training data! Cleaning...")
            x_train = np.nan_to_num(x_train, nan=0.0, posinf=1.0, neginf=-1.0)
        
        if np.isnan(x_test).any() or np.isinf(x_test).any():
            print("WARNING: NaN or Inf values found in test data! Cleaning...")
            x_test = np.nan_to_num(x_test, nan=0.0, posinf=1.0, neginf=-1.0)
    else:
        raise Exception(f'Dataset "{dataset}" not available.')

    log_dir = f'{output_path}/logs'
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    save_path = f"{output_path}/{id}"
    os.makedirs(save_path, exist_ok=True)

    ############################### TLCC ì‚°ì¶œ ë° ìƒê´€ê´€ê³„ë¥¼ ë°˜ì˜í•œ x_train ìƒì„± ####################################

    one_count = []
    for i in y_test:
        if i == 1:
            one_count.append(i)

    
    data_df = pd.DataFrame(x_train)
    
    # TLCC threshold ì¸ì ë³€ìˆ˜ ì •ì˜
    tlcc_threshold = getattr(args, 'tlcc_threshold', None)

    # ì§„ì§œ TLCC ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_true_tlcc = getattr(args, 'use_true_tlcc', False)
    
    if use_true_tlcc:
        print("ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)")
        from true_tlcc_implementation import columns_by_max_cross_correlation_tlcc
        
        # ë°ì´í„°ì…‹ë³„ ìºì‹±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì „ë‹¬
        corr_adj_df = columns_by_max_cross_correlation_tlcc(
            data_df, 
            max_lag=10, 
            dataset_name=dataset,
            output_dir=output_path  # ì‹¤í—˜ ê²°ê³¼ì™€ ê°™ì€ í´ë”ì— ì €ì¥
        )
        corr_adj_np = corr_adj_df.values
    else:
        print("ğŸ“Š Using simple correlation (not true TLCC)")
        corr_adj_df = pd.DataFrame()
        for i in range(data_df.shape[1]):
            # ëŒ€ì²´ êµ¬í˜„ ì‚¬ìš©
            from correlation_alternative import cross_correlate
            corr = cross_correlate(data_df, data_df.iloc[:, i], lag_idx=1)
            corr_adj_df[i] = corr

        corr_adj_df = corr_adj_df.fillna(0)
        corr_adj_np = corr_adj_df.values
    
    corr_adj_np = np.nan_to_num(corr_adj_np)
    
    # TLCC threshold ë° binary ë³€í™˜ ì˜µì…˜
    tlcc_binary = getattr(args, 'tlcc_binary', False)
    
    # threshold ì ìš©: tlcc_thresholdê°€ Noneì´ ì•„ë‹ ë•Œë§Œ ì ìš©, Noneì´ë©´ ëª¨ë“  ìƒê´€ê³„ìˆ˜ ìœ ì§€
    if tlcc_threshold is not None:
        if tlcc_binary:
            # threshold ë„˜ìœ¼ë©´ 1ë¡œ ë³€í™˜, ì•ˆ ë„˜ìœ¼ë©´ 0ìœ¼ë¡œ ë³€í™˜
            print(f"ğŸ”¥ TLCC Binary Mode: threshold={tlcc_threshold}, aboveâ†’1, belowâ†’0")
            corr_adj_np = np.where(np.abs(corr_adj_np) >= tlcc_threshold, 1.0, 0.0)
        else:
            # threshold ë„˜ìœ¼ë©´ ì›ë˜ ìƒê´€ê³„ìˆ˜ ê°’ ìœ ì§€, ì•ˆ ë„˜ìœ¼ë©´ 0ìœ¼ë¡œ ë³€í™˜ (ê¸°ì¡´ ë°©ì‹)
            print(f"ğŸ“Š TLCC Value Mode: threshold={tlcc_threshold}, aboveâ†’original_value, belowâ†’0")
            corr_adj_np = np.where(np.abs(corr_adj_np) >= tlcc_threshold, corr_adj_np, 0)
    else:
        print("ğŸ“ˆ TLCC No Threshold: using all correlation values")
    
    np.fill_diagonal(corr_adj_np, 1)

    # íˆíŠ¸ë§µ ì €ì¥ (ìƒê´€ê³„ìˆ˜ ê°’ ê·¸ëŒ€ë¡œ, ë°˜ë“œì‹œ save_path ìƒì„± ì´í›„ì— ì €ì¥)
    import matplotlib.pyplot as plt
    import seaborn as sns
    plt.figure(figsize=(16, 12))
    sns.heatmap(corr_adj_df, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, cbar=True)
    plt.title(f"Correlation Matrix (TLCC, ì›ë³¸)")
    plt.tight_layout()
    plt.savefig(f"{save_path}/corr_adj_heatmap.png", dpi=300)
    plt.close()

    # threshold ì ìš©ëœ íˆíŠ¸ë§µ ì €ì¥ (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ì¸ì ‘í–‰ë ¬)
    plt.figure(figsize=(16, 12))
    sns.heatmap(pd.DataFrame(corr_adj_np), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1, cbar=True)
    
    if tlcc_threshold is not None:
        if tlcc_binary:
            title_suffix = f"Binary Mode (threshold={tlcc_threshold})"
        else:
            title_suffix = f"Value Mode (threshold={tlcc_threshold})"
    else:
        title_suffix = "no threshold (all correlations included)"
    
    plt.title(f"Adjacency Matrix Used in Model (TLCC, {title_suffix})")
    plt.tight_layout()
    plt.savefig(f"{save_path}/corr_adj_heatmap_thresholded.png", dpi=300)
    plt.close()
    
    # ì—°ê²°ì„± í†µê³„ ì¶œë ¥
    total_connections = np.count_nonzero(corr_adj_np) - corr_adj_np.shape[0]  # ëŒ€ê°ì„  ì œì™¸
    total_possible = corr_adj_np.shape[0] * corr_adj_np.shape[1] - corr_adj_np.shape[0]  # ëŒ€ê°ì„  ì œì™¸
    connection_ratio = total_connections / total_possible * 100
    
    print(f"\nğŸ“Š Adjacency Matrix Statistics:")
    print(f"   Matrix shape: {corr_adj_np.shape}")
    print(f"   Total connections: {total_connections}/{total_possible} ({connection_ratio:.1f}%)")
    if tlcc_threshold is not None:
        if tlcc_binary:
            print(f"   Mode: Binary (1 if |corr| >= {tlcc_threshold}, else 0)")
            print(f"   Values: {np.count_nonzero(corr_adj_np == 1)} ones, {np.count_nonzero(corr_adj_np == 0)} zeros")
        else:
            print(f"   Mode: Value (original corr if |corr| >= {tlcc_threshold}, else 0)")
            print(f"   Non-zero range: {corr_adj_np[corr_adj_np != 0].min():.4f} to {corr_adj_np[corr_adj_np != 0].max():.4f}")
    else:
        print(f"   Mode: All correlations (no threshold)")
        print(f"   Value range: {corr_adj_np.min():.4f} to {corr_adj_np.max():.4f}")

    # ì‹¤ì œ í•™ìŠµ ë°ì´í„°ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì¸ì ‘í–‰ë ¬ë§Œ ëª¨ë¸ì— ì „ë‹¬
    # x_train, x_testëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê³±ì…ˆ ì œê±°)
    
    x_train_df = pd.DataFrame(x_train)
    x_test_df = pd.DataFrame(x_test)
    y_test_df = pd.DataFrame(y_test)
    ###########################################

    x_train = torch.from_numpy(x_train).float()
    x_test = torch.from_numpy(x_test).float()

    
    n_features = x_train.shape[1]

    target_dims = get_target_dims(dataset)

    if target_dims is None:
        out_dim = n_features
        print(f"Will forecast and reconstruct all {n_features} input features")
    elif type(target_dims) == int:
        print(f"Will forecast and reconstruct input feature: {target_dims}")
        out_dim = 1
    else:
        print(f"Will forecast and reconstruct input features: {target_dims}")
        out_dim = len(target_dims)

    train_dataset = SlidingWindowDataset(x_train, window_size, target_dims)
    test_dataset = SlidingWindowDataset(x_test, window_size, target_dims)
    
    train_loader, val_loader, test_loader = create_data_loaders(
    train_dataset, batch_size, val_split, shuffle_dataset, test_dataset=test_dataset
    )
    ############## Robust #######################

    model = MTAD_GAT(
        n_features,
        window_size,
        out_dim,
        #corr_adg_tensor,
        corr_adj_np,
        kernel_size=args.kernel_size,
        use_gatv2=args.use_gatv2,
        feat_gat_embed_dim=args.feat_gat_embed_dim,
        time_gat_embed_dim=args.time_gat_embed_dim,
        gru_n_layers=args.gru_n_layers,
        gru_hid_dim=args.gru_hid_dim,
        forecast_n_layers=args.fc_n_layers,
        forecast_hid_dim=args.fc_hid_dim,
        recon_n_layers=args.recon_n_layers,
        recon_hid_dim=args.recon_hid_dim,
        dropout=args.dropout,
        alpha=args.alpha
    )
    print('Model:\n', model)
    
    # Check model weights for NaN/Inf
    for name, param in model.named_parameters():
        if torch.isnan(param).any() or torch.isinf(param).any():
            print(f"WARNING: NaN or Inf in model parameter {name}")
    
    # Test model with dummy input to ensure stability
    dummy_input = torch.randn(2, window_size, n_features) * 0.1
    try:
        with torch.no_grad():
            dummy_output = model(dummy_input)
            if torch.isnan(dummy_output).any() or torch.isinf(dummy_output).any():
                print("WARNING: Model produces NaN or Inf outputs!")
            else:
                print("Model forward pass test: PASSED")
    except Exception as e:
        print(f"Model forward pass test: FAILED - {e}")
        exit(1)

    # Use more stable optimizer settings
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=args.init_lr,
        weight_decay=1e-6,  # Add small weight decay for regularization
        eps=1e-8  # Increase epsilon for numerical stability
    )
    forecast_criterion = nn.MSELoss()
    recon_criterion = nn.MSELoss()

    trainer = Trainer(
        model,
        optimizer,
        window_size,
        n_features,
        target_dims,
        n_epochs,
        batch_size,
        init_lr,
        forecast_criterion,
        recon_criterion,
        use_cuda,
        save_path,
        log_dir,
        print_every,
        log_tensorboard,
        args_summary
    )
    
    import torch
    torch.autograd.set_detect_anomaly(True)  # â† ì´ ì¤„ ì¶”ê°€

    # í•™ìŠµ ì‹œì‘
    trainer.fit(train_loader, val_loader)

    plot_losses(trainer.losses, save_path=save_path, plot=False)

    # Check test loss
    test_loss = trainer.evaluate(test_loader)
    print(f"Test forecast loss: {test_loss[0]:.5f}")
    print(f"Test total loss: {test_loss[1]:.5f}")

    # Some suggestions for POT args
    level_q_dict = {
        "SMAP": (0.90, 0.005),
        "MSL": (0.90, 0.001),
        "SMD-1": (0.9950, 0.001),
        "SMD-2": (0.9925, 0.001),
        "SMD-3": (0.9999, 0.001),
        "WADI": (0.90, 0.001)
    }
    key = "SMD-" + args.group[0] if args.dataset == "SMD" else args.dataset
    level, q = level_q_dict[key]
    if args.level is not None:
        level = args.level
    if args.q is not None:
        q = args.q

    # Some suggestions for Epsilon args
    reg_level_dict = {"SMAP": 0, "MSL": 0, "SMD-1": 1, "SMD-2": 1, "SMD-3": 1, "WADI": 0}
    key = "SMD-" + args.group[0] if dataset == "SMD" else dataset
    reg_level = reg_level_dict[key]

    trainer.load(f"{save_path}/model.pt")
    prediction_args = {
        'dataset': dataset,
        "target_dims": target_dims,
        'scale_scores': args.scale_scores,
        "level": level,
        "q": q,
        'dynamic_pot': args.dynamic_pot,
        "use_mov_av": args.use_mov_av,
        "gamma": args.gamma,
        "reg_level": reg_level,
        "save_path": save_path,
        "use_wadi_optimization": (dataset.upper() == "WADI"),  # WADI ìµœì í™” ì¶”ê°€
    }
    best_model = trainer.model
    predictor = Predictor(
        best_model,
        window_size,
        n_features,
        prediction_args,
    )

    label = y_test[window_size:] if y_test is not None else None
    predictor.predict_anomalies(x_train, x_test, label)

    # Save config
    args_path = f"{save_path}/config.txt"
    with open(args_path, "w") as f:
        json.dump(args.__dict__, f, indent=2)
print('\n\n Total time : ', time.time() - start)