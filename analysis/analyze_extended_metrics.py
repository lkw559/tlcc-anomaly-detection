#!/usr/bin/env python3
"""
ê¸°ì¡´ ì‹¤í—˜ ê²°ê³¼ì— ì¶”ê°€ í‰ê°€ì§€í‘œ (ROC-AUC, PR-AUC, MCC) ì ìš©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import pickle
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from eval_methods import calculate_additional_metrics, print_extended_metrics

def load_experiment_data(experiment_dir):
    """ì‹¤í—˜ ê²°ê³¼ ë°ì´í„° ë¡œë“œ"""
    test_output_path = os.path.join(experiment_dir, 'test_output.pkl')
    config_path = os.path.join(experiment_dir, 'config.txt')
    summary_path = os.path.join(experiment_dir, 'summary.txt')
    
    if not all(os.path.exists(p) for p in [test_output_path, config_path, summary_path]):
        return None
    
    try:
        with open(test_output_path, 'rb') as f:
            test_output = pickle.load(f)
        with open(config_path, 'r') as f:
            config = json.load(f)
        with open(summary_path, 'r') as f:
            summary = json.load(f)
        
        return test_output, config, summary
    except Exception as e:
        print(f"Error loading {experiment_dir}: {e}")
        return None

def calculate_extended_metrics_for_experiment(experiment_dir):
    """ë‹¨ì¼ ì‹¤í—˜ì— ëŒ€í•´ í™•ì¥ëœ ì§€í‘œ ê³„ì‚°"""
    
    data = load_experiment_data(experiment_dir)
    if data is None:
        return None
    
    test_output, config, summary = data
    
    # í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
    if 'A_Score_Global' not in test_output or 'A_True_Global' not in test_output:
        print(f"Warning: Missing required data in {experiment_dir}")
        return None
    
    scores = test_output['A_Score_Global']
    labels = test_output['A_True_Global']
    
    # ê¸°ì¡´ ì„ê³„ê°’ ì‚¬ìš©
    epsilon_threshold = summary['epsilon_result']['threshold']
    
    # ì˜ˆì¸¡ê°’ ê³„ì‚°
    from eval_methods import adjust_predicts
    pred_result = adjust_predicts(scores, labels, epsilon_threshold, calc_latency=False)
    if isinstance(pred_result, tuple):
        pred = pred_result[0]
    else:
        pred = pred_result
    
    # ì¶”ê°€ ì§€í‘œ ê³„ì‚°
    additional_metrics = calculate_additional_metrics(labels, pred, scores)
    
    # ê²°ê³¼ í†µí•©
    result = {
        'experiment': os.path.basename(experiment_dir),
        'comment': config.get('comment', 'N/A'),
        'tlcc_threshold': config.get('tlcc_threshold', 1.0),
        'epochs': config.get('epochs', 1),
        'tlcc_binary': config.get('tlcc_binary', False),
        
        # ê¸°ì¡´ ì§€í‘œ
        'f1': summary['epsilon_result']['f1'],
        'precision': summary['epsilon_result']['precision'],
        'recall': summary['epsilon_result']['recall'],
        'TP': summary['epsilon_result']['TP'],
        'FP': summary['epsilon_result']['FP'],
        'TN': summary['epsilon_result']['TN'],
        'FN': summary['epsilon_result']['FN'],
        'threshold': epsilon_threshold,
        
        # ì¶”ê°€ ì§€í‘œ
        **additional_metrics
    }
    
    return result

def analyze_all_experiments():
    """ëª¨ë“  WADI ì‹¤í—˜ì— ëŒ€í•´ í™•ì¥ëœ ì§€í‘œ ë¶„ì„"""
    
    base_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/output/WADI/'
    experiment_dirs = [d for d in os.listdir(base_path) 
                      if os.path.isdir(os.path.join(base_path, d)) and d != 'logs']
    experiment_dirs.sort(reverse=True)  # ìµœì‹ ìˆœ
    
    print("ğŸ” ëª¨ë“  WADI ì‹¤í—˜ì— í™•ì¥ëœ í‰ê°€ì§€í‘œ ì ìš© ì¤‘...")
    print(f"ì´ {len(experiment_dirs)}ê°œì˜ ì‹¤í—˜ ë°œê²¬")
    
    results = []
    
    for i, exp_dir in enumerate(experiment_dirs):
        full_path = os.path.join(base_path, exp_dir)
        print(f"\n{i+1}/{len(experiment_dirs)} ì²˜ë¦¬ ì¤‘: {exp_dir}")
        
        result = calculate_extended_metrics_for_experiment(full_path)
        if result:
            results.append(result)
            
            # ì£¼ìš” ì§€í‘œ ì¶œë ¥
            print(f"   F1: {result['f1']:.4f}, ROC-AUC: {result['roc_auc']:.4f}, "
                  f"PR-AUC: {result['pr_auc']:.4f}, MCC: {result['mcc']:.4f}")
        else:
            print(f"   ì‹¤íŒ¨: ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜")
    
    if not results:
        print("ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    return pd.DataFrame(results)

def create_extended_metrics_comparison(df):
    """í™•ì¥ëœ ì§€í‘œë¡œ ì‹¤í—˜ ë¹„êµ ì‹œê°í™”"""
    
    if len(df) < 2:
        print("ì‹œê°í™”ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•œê¸€ í°íŠ¸ ì„¤ì • (ì˜µì…˜)
    plt.rcParams['font.family'] = 'DejaVu Sans'
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('í™•ì¥ëœ í‰ê°€ì§€í‘œë¡œ ë³¸ WADI ì‹¤í—˜ ê²°ê³¼ ë¹„êµ', fontsize=16)
    
    # 1. TLCC thresholdë³„ F1 vs ROC-AUC
    ax1 = axes[0, 0]
    for threshold in sorted(df['tlcc_threshold'].unique()):
        subset = df[df['tlcc_threshold'] == threshold]
        ax1.scatter(subset['f1'], subset['roc_auc'], 
                   label=f'TLCC {threshold}', alpha=0.7, s=100)
    ax1.set_xlabel('F1 Score')
    ax1.set_ylabel('ROC-AUC')
    ax1.set_title('F1 vs ROC-AUC')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. TLCC thresholdë³„ F1 vs PR-AUC
    ax2 = axes[0, 1]
    for threshold in sorted(df['tlcc_threshold'].unique()):
        subset = df[df['tlcc_threshold'] == threshold]
        ax2.scatter(subset['f1'], subset['pr_auc'], 
                   label=f'TLCC {threshold}', alpha=0.7, s=100)
    ax2.set_xlabel('F1 Score')
    ax2.set_ylabel('PR-AUC')
    ax2.set_title('F1 vs PR-AUC')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. TLCC thresholdë³„ F1 vs MCC
    ax3 = axes[0, 2]
    for threshold in sorted(df['tlcc_threshold'].unique()):
        subset = df[df['tlcc_threshold'] == threshold]
        ax3.scatter(subset['f1'], subset['mcc'], 
                   label=f'TLCC {threshold}', alpha=0.7, s=100)
    ax3.set_xlabel('F1 Score')
    ax3.set_ylabel('MCC')
    ax3.set_title('F1 vs MCC')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ì§€í‘œë³„ ë°•ìŠ¤í”Œë¡¯
    metrics_to_plot = ['f1', 'roc_auc', 'pr_auc', 'mcc']
    metric_names = ['F1 Score', 'ROC-AUC', 'PR-AUC', 'MCC']
    
    for i, (metric, name) in enumerate(zip(metrics_to_plot, metric_names)):
        if i < 3:  # ì²« ë²ˆì§¸ í–‰ì˜ ë‚˜ë¨¸ì§€ ê³µê°„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            continue
        ax = axes[1, i-3] if i >= 3 else axes[1, i]
        
        # TLCC thresholdë³„ ë°•ìŠ¤í”Œë¡¯
        data_for_box = []
        labels_for_box = []
        
        for threshold in sorted(df['tlcc_threshold'].unique()):
            subset = df[df['tlcc_threshold'] == threshold]
            if len(subset) > 0:
                data_for_box.append(subset[metric].values)
                labels_for_box.append(f'TLCC {threshold}')
        
        if data_for_box:
            ax.boxplot(data_for_box, labels=labels_for_box)
            ax.set_title(f'{name} by TLCC Threshold')
            ax.set_ylabel(name)
            ax.grid(True, alpha=0.3)
    
    # 5. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    ax5 = axes[1, 0]
    correlation_metrics = ['f1', 'precision', 'recall', 'roc_auc', 'pr_auc', 'mcc']
    corr_data = df[correlation_metrics].corr()
    
    sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, ax=ax5)
    ax5.set_title('ì§€í‘œ ê°„ ìƒê´€ê´€ê³„')
    
    # 6. ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ (ìƒìœ„ 3ê°œ ì‹¤í—˜)
    ax6 = axes[1, 1]
    
    # ìƒìœ„ 3ê°œ ì‹¤í—˜ ì„ íƒ (F1 ê¸°ì¤€)
    top3 = df.nlargest(3, 'f1')
    
    # ë ˆì´ë” ì°¨íŠ¸ ëŒ€ì‹  ë°” ì°¨íŠ¸ë¡œ ëŒ€ì²´
    x_pos = np.arange(len(metrics_to_plot))
    width = 0.25
    
    for i, (idx, row) in enumerate(top3.iterrows()):
        values = [row[metric] for metric in metrics_to_plot]
        ax6.bar(x_pos + i*width, values, width, 
               label=f"{row['comment'][:20]}...", alpha=0.7)
    
    ax6.set_xlabel('Metrics')
    ax6.set_ylabel('Score')
    ax6.set_title('Top 3 Experiments Comparison')
    ax6.set_xticks(x_pos + width)
    ax6.set_xticklabels(metric_names, rotation=45)
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    # 7. TLCC threshold íš¨ê³¼ ìš”ì•½
    ax7 = axes[1, 2]
    
    tlcc_summary = df.groupby('tlcc_threshold')[metrics_to_plot].mean()
    
    x_pos = np.arange(len(tlcc_summary.index))
    width = 0.2
    
    for i, metric in enumerate(metrics_to_plot):
        ax7.bar(x_pos + i*width, tlcc_summary[metric], width, 
               label=metric_names[i], alpha=0.7)
    
    ax7.set_xlabel('TLCC Threshold')
    ax7.set_ylabel('Average Score')
    ax7.set_title('Average Performance by TLCC Threshold')
    ax7.set_xticks(x_pos + 1.5*width)
    ax7.set_xticklabels([f'TLCC {t}' for t in tlcc_summary.index])
    ax7.legend()
    ax7.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # íŒŒì¼ ì €ì¥
    output_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/extended_metrics_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š ì‹œê°í™” ê²°ê³¼ ì €ì¥: {output_path}")
    
    plt.show()

def print_ranking_by_metrics(df):
    """ê° ì§€í‘œë³„ ì‹¤í—˜ ìˆœìœ„ ì¶œë ¥"""
    
    metrics = ['f1', 'roc_auc', 'pr_auc', 'mcc']
    metric_names = ['F1 Score', 'ROC-AUC', 'PR-AUC', 'MCC']
    
    print(f"\nğŸ† ì§€í‘œë³„ ì‹¤í—˜ ìˆœìœ„ (ìƒìœ„ 3ê°œ)")
    print("=" * 80)
    
    for metric, name in zip(metrics, metric_names):
        print(f"\nğŸ“Š {name} ìˆœìœ„:")
        top3 = df.nlargest(3, metric)
        
        for i, (idx, row) in enumerate(top3.iterrows()):
            print(f"   {i+1}ìœ„. {row['comment'][:40]}...")
            print(f"        {name}: {row[metric]:.4f}")
            print(f"        TLCC: {row['tlcc_threshold']}, Epochs: {row['epochs']}")
            print(f"        F1: {row['f1']:.4f}, Precision: {row['precision']:.4f}, Recall: {row['recall']:.4f}")

def main():
    """ë©”ì¸ ë¶„ì„ í•¨ìˆ˜"""
    
    print("ğŸ”¬ ê¸°ì¡´ ì‹¤í—˜ ê²°ê³¼ì— í™•ì¥ëœ í‰ê°€ì§€í‘œ (ROC-AUC, PR-AUC, MCC) ì ìš©")
    print("=" * 80)
    
    # ëª¨ë“  ì‹¤í—˜ ë¶„ì„
    df = analyze_all_experiments()
    
    if df is None or len(df) == 0:
        print("ë¶„ì„í•  ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì´ {len(df)}ê°œ ì‹¤í—˜ ë¶„ì„ ì™„ë£Œ")
    
    # ìš”ì•½ í†µê³„
    print(f"\nğŸ“ˆ ì§€í‘œë³„ ìš”ì•½ í†µê³„:")
    key_metrics = ['f1', 'roc_auc', 'pr_auc', 'mcc']
    summary_stats = df[key_metrics].describe()
    print(summary_stats.round(4))
    
    # TLCC thresholdë³„ í‰ê·  ì„±ëŠ¥
    print(f"\nğŸ”— TLCC Thresholdë³„ í‰ê·  ì„±ëŠ¥:")
    tlcc_avg = df.groupby('tlcc_threshold')[key_metrics].mean()
    print(tlcc_avg.round(4))
    
    # ì§€í‘œë³„ ìˆœìœ„
    print_ranking_by_metrics(df)
    
    # ìµœê³  ì¢…í•© ì„±ëŠ¥ ì‹¤í—˜
    print(f"\nğŸ¥‡ ì¢…í•© ìµœê³  ì„±ëŠ¥ ì‹¤í—˜:")
    # ëª¨ë“  ì§€í‘œì˜ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
    df['composite_score'] = df[key_metrics].mean(axis=1)
    best_overall = df.loc[df['composite_score'].idxmax()]
    
    print(f"   ì‹¤í—˜: {best_overall['comment']}")
    print(f"   TLCC: {best_overall['tlcc_threshold']}, Epochs: {best_overall['epochs']}")
    print(f"   ì¢…í•©ì ìˆ˜: {best_overall['composite_score']:.4f}")
    print(f"   F1: {best_overall['f1']:.4f}, ROC-AUC: {best_overall['roc_auc']:.4f}")
    print(f"   PR-AUC: {best_overall['pr_auc']:.4f}, MCC: {best_overall['mcc']:.4f}")
    
    # ì‹œê°í™”
    create_extended_metrics_comparison(df)
    
    # ê²°ê³¼ ì €ì¥
    output_csv = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/extended_metrics_results.csv'
    df.to_csv(output_csv, index=False)
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_csv}")
    
    print(f"\nğŸ‰ í™•ì¥ëœ í‰ê°€ì§€í‘œ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
