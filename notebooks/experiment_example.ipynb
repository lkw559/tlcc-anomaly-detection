{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1a4352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ì‚¬ìš© ê°€ëŠ¥ (GPU ê°œìˆ˜: 8)\n",
      "í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU: 0 - NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥ (GPU ê°œìˆ˜: {torch.cuda.device_count()})\")\n",
    "    print(f\"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU: {torch.cuda.current_device()} - {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA ì‚¬ìš© ë¶ˆê°€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f283e001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: SMAP\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (135183, 25)\n",
      "test set shape:  (427617, 25)\n",
      "test set label shape:  (427617,)\n",
      "Data normalized\n",
      "train set shape:  (135183, 25)\n",
      "test set shape:  (427617, 25)\n",
      "test set label shape:  (427617,)\n",
      "ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)\n",
      "ğŸ“ Loading cached TLCC results from output/SMAP/tlcc_correlation_matrix_SMAP_lag10.csv\n",
      "ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)\n",
      "ğŸ“ Loading cached TLCC results from output/SMAP/tlcc_correlation_matrix_SMAP_lag10.csv\n",
      "ğŸ“ Adjusting cached column names to match current data...\n",
      "âœ… Successfully loaded cached TLCC matrix. Shape: (25, 25)\n",
      "TLCC correlation range: 0.0000 to 1.0000\n",
      "Lag range: -10 to 10\n",
      "ğŸ”¥ TLCC Binary Mode: threshold=0.8, aboveâ†’1, belowâ†’0\n",
      "ğŸ“ Adjusting cached column names to match current data...\n",
      "âœ… Successfully loaded cached TLCC matrix. Shape: (25, 25)\n",
      "TLCC correlation range: 0.0000 to 1.0000\n",
      "Lag range: -10 to 10\n",
      "ğŸ”¥ TLCC Binary Mode: threshold=0.8, aboveâ†’1, belowâ†’0\n",
      "\n",
      "ğŸ“Š Adjacency Matrix Statistics:\n",
      "   Matrix shape: (25, 25)\n",
      "   Total connections: 12/600 (2.0%)\n",
      "   Mode: Binary (1 if |corr| >= 0.8, else 0)\n",
      "   Values: 37 ones, 588 zeros\n",
      "Will forecast and reconstruct input features: [0]\n",
      "train_sampler: <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f23f38bc9a0>\n",
      "train_size: 121575\n",
      "validation_size: 13508\n",
      "test_size: 427517\n",
      "\n",
      "ğŸ“Š Adjacency Matrix Statistics:\n",
      "   Matrix shape: (25, 25)\n",
      "   Total connections: 12/600 (2.0%)\n",
      "   Mode: Binary (1 if |corr| >= 0.8, else 0)\n",
      "   Values: 37 ones, 588 zeros\n",
      "Will forecast and reconstruct input features: [0]\n",
      "train_sampler: <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f23f38bc9a0>\n",
      "train_size: 121575\n",
      "validation_size: 13508\n",
      "test_size: 427517\n",
      "Model:\n",
      " MTAD_GAT(\n",
      "  (conv): ConvLayer(\n",
      "    (padding): ConstantPad1d(padding=(3, 3), value=0.0)\n",
      "    (conv): Conv1d(25, 25, kernel_size=(7,), stride=(1,))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (weight): WeightLayer(\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (gru): GRULayer(\n",
      "    (gru): GRU(25, 64, batch_first=True)\n",
      "  )\n",
      "  (forecasting_model): Forecasting_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Model forward pass test: PASSED\n",
      "Model:\n",
      " MTAD_GAT(\n",
      "  (conv): ConvLayer(\n",
      "    (padding): ConstantPad1d(padding=(3, 3), value=0.0)\n",
      "    (conv): Conv1d(25, 25, kernel_size=(7,), stride=(1,))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (weight): WeightLayer(\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (gru): GRULayer(\n",
      "    (gru): GRU(25, 64, batch_first=True)\n",
      "  )\n",
      "  (forecasting_model): Forecasting_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Model forward pass test: PASSED\n",
      "Init total train loss: 0.320706\n",
      "Init total train loss: 0.320706\n",
      "Init total val loss: 0.31610\n",
      "Training model for 2 epochs..\n",
      "Init total val loss: 0.31610\n",
      "Training model for 2 epochs..\n",
      "[Epoch 1] forecast_loss = 0.05847, total_loss = 0.05847 ---- val_forecast_loss = 0.00998, val_total_loss = 0.00998 [70.6s]\n",
      "[Epoch 1] forecast_loss = 0.05847, total_loss = 0.05847 ---- val_forecast_loss = 0.00998, val_total_loss = 0.00998 [70.6s]\n",
      "[Epoch 2] forecast_loss = 0.01250, total_loss = 0.01250 ---- val_forecast_loss = 0.00739, val_total_loss = 0.00739 [72.2s]\n",
      "-- Training done in 142s.\n",
      "[Epoch 2] forecast_loss = 0.01250, total_loss = 0.01250 ---- val_forecast_loss = 0.00739, val_total_loss = 0.00739 [72.2s]\n",
      "-- Training done in 142s.\n",
      "Test forecast loss: 0.00678\n",
      "Test total loss: 0.00678\n",
      "Predicting and calculating anomaly scores..\n",
      "  0%|                                                   | 0/528 [00:00<?, ?it/s]Test forecast loss: 0.00678\n",
      "Test total loss: 0.00678\n",
      "Predicting and calculating anomaly scores..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528/528 [00:03<00:00, 134.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528/528 [00:03<00:00, 134.26it/s]\n",
      "Predicting and calculating anomaly scores..\n",
      "  0%|                                                  | 0/1670 [00:00<?, ?it/s]Predicting and calculating anomaly scores..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1670/1670 [00:11<00:00, 146.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1670/1670 [00:11<00:00, 146.01it/s]\n",
      "------------------------------------\n",
      "train_pred_df.shape: (135083, 4)\n",
      "test_pred_df.shape: (427517, 4)\n",
      "------------------------------------\n",
      "train_anomaly_scores: 135083\n",
      "test_anomaly_scores: 427517\n",
      "------------------------------------\n",
      "train_pred_df.shape: (135083, 4)\n",
      "test_pred_df.shape: (427517, 4)\n",
      "------------------------------------\n",
      "train_anomaly_scores: 135083\n",
      "test_anomaly_scores: 427517\n",
      "Results using epsilon method (standard):\n",
      " {'f1': 0.7313422260898752, 'precision': 0.9869536852772486, 'recall': 0.5809017111706702, 'TP': 31773, 'TN': 372401, 'FP': 420, 'FN': 22923, 'threshold': 0.9327415823936462, 'latency': 363.3103037548805, 'reg_level': 1}\n",
      "Running POT with q=0.005, level=0.9..\n",
      "Initial threshold : 0.23046766\n",
      "Number of peaks : 13507\n",
      "Grimshaw maximum log-likelihood estimation ... Results using epsilon method (standard):\n",
      " {'f1': 0.7313422260898752, 'precision': 0.9869536852772486, 'recall': 0.5809017111706702, 'TP': 31773, 'TN': 372401, 'FP': 420, 'FN': 22923, 'threshold': 0.9327415823936462, 'latency': 363.3103037548805, 'reg_level': 1}\n",
      "Running POT with q=0.005, level=0.9..\n",
      "Initial threshold : 0.23046766\n",
      "Number of peaks : 13507\n",
      "Grimshaw maximum log-likelihood estimation ... [done]\n",
      "\tÎ³ = -0.22799457609653473\n",
      "\tÏƒ = 0.3475368325795625\n",
      "\tL = 3847.8635062734684\n",
      "Extreme quantile (probability = 0.005): 0.9848461721074782\n",
      "  0%|                                                | 0/427517 [00:00<?, ?it/s][done]\n",
      "\tÎ³ = -0.22799457609653473\n",
      "\tÏƒ = 0.3475368325795625\n",
      "\tL = 3847.8635062734684\n",
      "Extreme quantile (probability = 0.005): 0.9848461721074782\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427517/427517 [00:00<00:00, 3452309.12it/s]\n",
      "0\n",
      "427517\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427517/427517 [00:00<00:00, 3452309.12it/s]\n",
      "0\n",
      "427517\n",
      "Results using POT method (standard):\n",
      " {'f1': 0.6779526231673284, 'precision': 0.9935743535540269, 'recall': 0.5145166007542569, 'TP': 28142, 'TN': 372639, 'FP': 182, 'FN': 26554, 'threshold': 0.984846172107479, 'latency': 329.30873974109585}\n",
      "Results using POT method (standard):\n",
      " {'f1': 0.6779526231673284, 'precision': 0.9935743535540269, 'recall': 0.5145166007542569, 'TP': 28142, 'TN': 372639, 'FP': 182, 'FN': 26554, 'threshold': 0.984846172107479, 'latency': 329.30873974109585}\n",
      "Saving output to output/SMAP/19062025_010921/<train/test>_output.pkl\n",
      "-- Done.\n",
      "\n",
      "\n",
      " Total time :  226.63622760772705\n",
      "Saving output to output/SMAP/19062025_010921/<train/test>_output.pkl\n",
      "-- Done.\n",
      "\n",
      "\n",
      " Total time :  226.63622760772705\n"
     ]
    }
   ],
   "source": [
    "!python train_original.py --comment 'SMD_tlcc_0.8_binary_0_epoch_10' --epoch 2 --bs 128 --dataset SMAP --tlcc_threshold 0.8 --use_true_tlcc 1 --tlcc_binary 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528dfae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: machine-1-1\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (28479, 38)\n",
      "test set shape:  (28479, 38)\n",
      "test set label shape:  (28479,)\n",
      "Data normalized\n",
      "train set shape:  (28479, 38)\n",
      "test set shape:  (28479, 38)\n",
      "test set label shape:  (28479,)\n",
      "ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)\n",
      "Computing TRUE Time-Lagged Cross-Correlations (max_lag=10)...\n",
      "ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)\n",
      "Computing TRUE Time-Lagged Cross-Correlations (max_lag=10)...\n",
      "  Processed 10/38 features...\n",
      "  Processed 10/38 features...\n",
      "  Processed 20/38 features...\n",
      "  Processed 20/38 features...\n",
      "  Processed 30/38 features...\n",
      "  Processed 30/38 features...\n",
      "TLCC matrix computed. Shape: (38, 38)\n",
      "TLCC correlation range: 0.0000 to 1.0000\n",
      "Lag range: -10 to 10\n",
      "Optimal lag matrix saved to tlcc_lag_matrix.csv\n",
      "TLCC matrix computed. Shape: (38, 38)\n",
      "TLCC correlation range: 0.0000 to 1.0000\n",
      "Lag range: -10 to 10\n",
      "Optimal lag matrix saved to tlcc_lag_matrix.csv\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/train_original.py\", line 164, in <module>\n",
      "    plt.savefig(f\"{save_path}/corr_adj_heatmap_thresholded.png\", dpi=300)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 1229, in savefig\n",
      "    fig.canvas.draw_idle()  # Need this if 'transparent=True', to reset colors.\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py\", line 1905, in draw_idle\n",
      "    self.draw(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 387, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 95, in draw_wrapper\n",
      "    result = draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py\", line 3162, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 3137, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/text.py\", line 751, in draw\n",
      "    bbox, info, descent = self._get_layout(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/text.py\", line 505, in _get_layout\n",
      "    xys = M.transform(offset_layout) - (offsetx, offsety)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/transforms.py\", line 1794, in transform\n",
      "    return self.transform_affine(values)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 300, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/transforms.py\", line 1865, in transform_affine\n",
      "    return affine_transform(values, mtx)\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/train_original.py\", line 164, in <module>\n",
      "    plt.savefig(f\"{save_path}/corr_adj_heatmap_thresholded.png\", dpi=300)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py\", line 1229, in savefig\n",
      "    fig.canvas.draw_idle()  # Need this if 'transparent=True', to reset colors.\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py\", line 1905, in draw_idle\n",
      "    self.draw(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 387, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 95, in draw_wrapper\n",
      "    result = draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py\", line 3162, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py\", line 3137, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py\", line 72, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/text.py\", line 751, in draw\n",
      "    bbox, info, descent = self._get_layout(renderer)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/text.py\", line 505, in _get_layout\n",
      "    xys = M.transform(offset_layout) - (offsetx, offsety)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/transforms.py\", line 1794, in transform\n",
      "    return self.transform_affine(values)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/_api/deprecation.py\", line 300, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/matplotlib/transforms.py\", line 1865, in transform_affine\n",
      "    return affine_transform(values, mtx)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train_original.py --comment 'use TLCC_threshold 0.5' --epoch 5 --bs 128 --dataset SMD --tlcc_threshold 0.5 --use_true_tlcc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14f409ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‹¤í—˜ ì„¤ì •:\n",
      "- Dataset: WADI\n",
      "- TLCC threshold: 1.0\n",
      "- Use true TLCC: True\n",
      "- Comment: wadi_true_tlcc_threshold_1.0\n",
      "\n",
      "í˜„ì¬ ì„±ëŠ¥:\n",
      "- F1 Score: 0.2175\n",
      "- Precision: 0.1220\n",
      "- Recall: 1.0000\n",
      "- Detection Threshold: 0.070347\n",
      "- TP: 3139.0\n",
      "- FP: 22584.0\n",
      "- TN: 26019.0\n",
      "- FN: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# WADI ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ\n",
    "result_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/output/WADI/18062025_194435'\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ\n",
    "with open(f'{result_path}/test_output.pkl', 'rb') as f:\n",
    "    test_output = pickle.load(f)\n",
    "\n",
    "# ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "with open(f'{result_path}/config.txt', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# ìš”ì•½ ê²°ê³¼ ë¡œë“œ\n",
    "with open(f'{result_path}/summary.txt', 'r') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(\"í˜„ì¬ ì‹¤í—˜ ì„¤ì •:\")\n",
    "print(f\"- Dataset: {config['dataset']}\")\n",
    "print(f\"- TLCC threshold: {config['tlcc_threshold']}\")\n",
    "print(f\"- Use true TLCC: {config['use_true_tlcc']}\")\n",
    "print(f\"- Comment: {config['comment']}\")\n",
    "\n",
    "print(\"\\ní˜„ì¬ ì„±ëŠ¥:\")\n",
    "print(f\"- F1 Score: {summary['epsilon_result']['f1']:.4f}\")\n",
    "print(f\"- Precision: {summary['epsilon_result']['precision']:.4f}\")\n",
    "print(f\"- Recall: {summary['epsilon_result']['recall']:.4f}\")\n",
    "print(f\"- Detection Threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "print(f\"- TP: {summary['epsilon_result']['TP']}\")\n",
    "print(f\"- FP: {summary['epsilon_result']['FP']}\")\n",
    "print(f\"- TN: {summary['epsilon_result']['TN']}\")\n",
    "print(f\"- FN: {summary['epsilon_result']['FN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# ëª¨ë“  WADI ì‹¤í—˜ ê²°ê³¼ í´ë” ì°¾ê¸°\n",
    "result_base_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/output/WADI/'\n",
    "result_folders = [f for f in os.listdir(result_base_path) if os.path.isdir(os.path.join(result_base_path, f)) and f != 'logs']\n",
    "\n",
    "# ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "result_folders.sort(reverse=True)\n",
    "\n",
    "print(f\"ì´ {len(result_folders)}ê°œì˜ ì‹¤í—˜ ê²°ê³¼ ë°œê²¬:\")\n",
    "for i, folder in enumerate(result_folders[:5]):  # ìµœì‹  5ê°œë§Œ í‘œì‹œ\n",
    "    config_path = os.path.join(result_base_path, folder, 'config.txt')\n",
    "    summary_path = os.path.join(result_base_path, folder, 'summary.txt')\n",
    "    \n",
    "    if os.path.exists(config_path) and os.path.exists(summary_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\n{i+1}. {folder}\")\n",
    "        print(f\"   Comment: {config.get('comment', 'N/A')}\")\n",
    "        print(f\"   Epochs: {config.get('epochs', 'N/A')}, TLCC threshold: {config.get('tlcc_threshold', 'N/A')}\")\n",
    "        print(f\"   F1: {summary['epsilon_result']['f1']:.4f}, Precision: {summary['epsilon_result']['precision']:.4f}, Recall: {summary['epsilon_result']['recall']:.4f}\")\n",
    "\n",
    "# ê°€ì¥ ìµœì‹  ì‹¤í—˜ ê²°ê³¼ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "if result_folders:\n",
    "    latest_result_path = os.path.join(result_base_path, result_folders[0])\n",
    "    print(f\"\\në¶„ì„í•  ì‹¤í—˜ ê²½ë¡œ: {latest_result_path}\")\n",
    "else:\n",
    "    print(\"\\nì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜ í™•ì¸\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def check_running_experiments():\n",
    "    \"\"\"python train_original.py í”„ë¡œì„¸ìŠ¤ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)\n",
    "        lines = result.stdout.split('\\n')\n",
    "        \n",
    "        running_experiments = []\n",
    "        for line in lines:\n",
    "            if 'train_original.py' in line and 'python' in line:\n",
    "                # ëª…ë ¹ì–´ì—ì„œ comment ì¶”ì¶œ\n",
    "                if '--comment' in line:\n",
    "                    comment_start = line.find('--comment') + len('--comment')\n",
    "                    comment_part = line[comment_start:].strip().split()[0]\n",
    "                    comment = comment_part.strip(\"'\\\"\")\n",
    "                    running_experiments.append(comment)\n",
    "                else:\n",
    "                    running_experiments.append('ì•Œ ìˆ˜ ì—†ëŠ” ì‹¤í—˜')\n",
    "        \n",
    "        return running_experiments\n",
    "    except Exception as e:\n",
    "        print(f\"í”„ë¡œì„¸ìŠ¤ í™•ì¸ ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "\n",
    "running_experiments = check_running_experiments()\n",
    "if running_experiments:\n",
    "    print(f\"í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜: {len(running_experiments)}ê°œ\")\n",
    "    for i, exp in enumerate(running_experiments, 1):\n",
    "        print(f\"  {i}. {exp}\")\n",
    "else:\n",
    "    print(\"í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nì˜ˆìƒ ì™„ë£Œ ì‹œê°„: 5 epoch ì‹¤í—˜ ì•½ 15-20ë¶„, 3 epoch ì‹¤í—˜ ì•½ 10-15ë¶„\")\n",
    "print(\"ëŒ€ê¸° ì¤‘...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4698a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì™„ë£Œëœ ì‹¤í—˜ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_all_experiments():\n",
    "    \"\"\"WADI ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„\"\"\"\n",
    "    experiments_data = []\n",
    "    \n",
    "    for folder in result_folders:\n",
    "        config_path = os.path.join(result_base_path, folder, 'config.txt')\n",
    "        summary_path = os.path.join(result_base_path, folder, 'summary.txt')\n",
    "        \n",
    "        if os.path.exists(config_path) and os.path.exists(summary_path):\n",
    "            try:\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                with open(summary_path, 'r') as f:\n",
    "                    summary = json.load(f)\n",
    "                \n",
    "                exp_data = {\n",
    "                    'timestamp': folder,\n",
    "                    'comment': config.get('comment', 'N/A'),\n",
    "                    'epochs': config.get('epochs', 1),\n",
    "                    'tlcc_threshold': config.get('tlcc_threshold', 1.0),\n",
    "                    'tlcc_binary': config.get('tlcc_binary', False),\n",
    "                    'use_true_tlcc': config.get('use_true_tlcc', False),\n",
    "                    'f1_score': summary['epsilon_result']['f1'],\n",
    "                    'precision': summary['epsilon_result']['precision'],\n",
    "                    'recall': summary['epsilon_result']['recall'],\n",
    "                    'TP': summary['epsilon_result']['TP'],\n",
    "                    'FP': summary['epsilon_result']['FP'],\n",
    "                    'TN': summary['epsilon_result']['TN'],\n",
    "                    'FN': summary['epsilon_result']['FN'],\n",
    "                    'threshold': summary['epsilon_result']['threshold']\n",
    "                }\n",
    "                experiments_data.append(exp_data)\n",
    "            except Exception as e:\n",
    "                print(f\"í´ë” {folder} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    if not experiments_data:\n",
    "        print(\"ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(experiments_data)\n",
    "    df = df.sort_values('timestamp', ascending=False)  # ìµœì‹ ìˆœ ì •ë ¬\n",
    "    \n",
    "    print(\"=== WADI ë°ì´í„°ì…‹ ì‹¤í—— ê²°ê³¼ ë¹„êµ ===\")\n",
    "    print(f\"ì´ {len(df)}ê°œì˜ ì‹¤í—˜ ê²°ê³¼\")\n",
    "    \n",
    "    # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ìš”ì•½\n",
    "    performance_cols = ['f1_score', 'precision', 'recall']\n",
    "    print(\"\\nìµœì‹  3ê°œ ì‹¤í—˜ ì„±ëŠ¥:\")\n",
    "    for i, (idx, row) in enumerate(df.head(3).iterrows()):\n",
    "        print(f\"  {i+1}. {row['comment'][:30]}...\")\n",
    "        print(f\"     F1: {row['f1_score']:.4f}, Prec: {row['precision']:.4f}, Rec: {row['recall']:.4f}\")\n",
    "        print(f\"     Epochs: {row['epochs']}, TLCC threshold: {row['tlcc_threshold']}\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°\n",
    "    best_f1_idx = df['f1_score'].idxmax()\n",
    "    best_exp = df.loc[best_f1_idx]\n",
    "    print(f\"\\nìµœê³  F1 Score ì‹¤í—˜:\")\n",
    "    print(f\"  Comment: {best_exp['comment']}\")\n",
    "    print(f\"  F1: {best_exp['f1_score']:.4f}, Precision: {best_exp['precision']:.4f}, Recall: {best_exp['recall']:.4f}\")\n",
    "    print(f\"  Settings: epochs={best_exp['epochs']}, tlcc_threshold={best_exp['tlcc_threshold']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "exp_df = analyze_all_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db000b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLCC thresholdë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
    "if exp_df is not None and len(exp_df) > 0:\n",
    "    # TLCC thresholdë³„ ê·¸ë£¹í™”\n",
    "    tlcc_comparison = exp_df.groupby('tlcc_threshold').agg({\n",
    "        'f1_score': ['mean', 'max', 'std'],\n",
    "        'precision': ['mean', 'max', 'std'],\n",
    "        'recall': ['mean', 'max', 'std'],\n",
    "        'epochs': 'first'  # epochsëŠ” ì°¸ê³ ìš©\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\n=== TLCC Thresholdë³„ ì„±ëŠ¥ ë¹„êµ ===\")\n",
    "    print(tlcc_comparison)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    if len(exp_df) >= 3:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í”Œë¡¯\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # F1 Score ë¹„êµ\n",
    "        exp_df.boxplot(column='f1_score', by='tlcc_threshold', ax=axes[0])\n",
    "        axes[0].set_title('F1 Score by TLCC Threshold')\n",
    "        axes[0].set_xlabel('TLCC Threshold')\n",
    "        axes[0].set_ylabel('F1 Score')\n",
    "        \n",
    "        # Precision vs Recall ì‚°ì ë„\n",
    "        for threshold in exp_df['tlcc_threshold'].unique():\n",
    "            subset = exp_df[exp_df['tlcc_threshold'] == threshold]\n",
    "            axes[1].scatter(subset['precision'], subset['recall'], \n",
    "                           label=f'TLCC {threshold}', alpha=0.7, s=100)\n",
    "        axes[1].set_xlabel('Precision')\n",
    "        axes[1].set_ylabel('Recall')\n",
    "        axes[1].set_title('Precision vs Recall by TLCC Threshold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # FP vs FN ë¹„êµ\n",
    "        for threshold in exp_df['tlcc_threshold'].unique():\n",
    "            subset = exp_df[exp_df['tlcc_threshold'] == threshold]\n",
    "            axes[2].scatter(subset['FP'], subset['FN'], \n",
    "                           label=f'TLCC {threshold}', alpha=0.7, s=100)\n",
    "        axes[2].set_xlabel('False Positives')\n",
    "        axes[2].set_ylabel('False Negatives')\n",
    "        axes[2].set_title('FP vs FN by TLCC Threshold')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # ì—°ê²° ë°€ë„ì™€ ì„±ëŠ¥ì˜ ê´€ê³„ ë¶„ì„\n",
    "    print(\"\\n=== ì—°ê²° ë°€ë„ì™€ ì„±ëŠ¥ ê´€ê³„ ë¶„ì„ ===\")\n",
    "    connection_density = {\n",
    "        0.8: \"1.2% (80/6642)\",\n",
    "        0.6: \"3.9% (256/6642)\", \n",
    "        0.5: \"6.9% (460/6642)\"\n",
    "    }\n",
    "    \n",
    "    for threshold in sorted(exp_df['tlcc_threshold'].unique()):\n",
    "        subset = exp_df[exp_df['tlcc_threshold'] == threshold]\n",
    "        if len(subset) > 0:\n",
    "            best_f1 = subset['f1_score'].max()\n",
    "            density = connection_density.get(threshold, \"N/A\")\n",
    "            print(f\"TLCC {threshold}: ì—°ê²°ë°€ë„ {density} â†’ ìµœê³  F1 {best_f1:.4f}\")\n",
    "else:\n",
    "    print(\"ë¶„ì„í•  ì‹¤í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def monitor_experiments(check_interval=30):\n",
    "    \"\"\"ì‹¤í—˜ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    print(f\"í˜„ì¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘... (Ctrl+Cë¡œ ì¤‘ë‹¨ ê°€ëŠ¥)\")\n",
    "    \n",
    "    last_folder_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # ìƒˆë¡œìš´ ê²°ê³¼ í´ë” í™•ì¸\n",
    "            current_folders = [f for f in os.listdir(result_base_path) \n",
    "                             if os.path.isdir(os.path.join(result_base_path, f)) and f != 'logs']\n",
    "            current_count = len(current_folders)\n",
    "            \n",
    "            if current_count > last_folder_count:\n",
    "                new_folders = current_count - last_folder_count\n",
    "                print(f\"\\nğŸ‰ {new_folders}ê°œì˜ ìƒˆë¡œìš´ ì‹¤í—˜ ì™„ë£Œ ê°ì§€!\")\n",
    "                \n",
    "                # ìµœì‹  ê²°ê³¼ ë¶„ì„\n",
    "                latest_folders = sorted(current_folders, reverse=True)[:new_folders]\n",
    "                for folder in latest_folders:\n",
    "                    try:\n",
    "                        config_path = os.path.join(result_base_path, folder, 'config.txt')\n",
    "                        summary_path = os.path.join(result_base_path, folder, 'summary.txt')\n",
    "                        \n",
    "                        if os.path.exists(config_path) and os.path.exists(summary_path):\n",
    "                            with open(config_path, 'r') as f:\n",
    "                                config = json.load(f)\n",
    "                            with open(summary_path, 'r') as f:\n",
    "                                summary = json.load(f)\n",
    "                            \n",
    "                            print(f\"  âœ“ {folder}\")\n",
    "                            print(f\"    Comment: {config.get('comment', 'N/A')}\")\n",
    "                            print(f\"    F1: {summary['epsilon_result']['f1']:.4f}\")\n",
    "                            print(f\"    TLCC threshold: {config.get('tlcc_threshold', 'N/A')}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ì˜¤ë¥˜: {e}\")\n",
    "                \n",
    "                last_folder_count = current_count\n",
    "            \n",
    "            # ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜ í™•ì¸\n",
    "            running = check_running_experiments()\n",
    "            if running:\n",
    "                print(f\"\\rí˜„ì¬: {datetime.now().strftime('%H:%M:%S')} | ì§„í–‰ì¤‘: {len(running)}ê°œ | ì™„ë£Œ: {current_count}ê°œ\", end=\"\")\n",
    "            else:\n",
    "                print(f\"\\n\\nğŸ† ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ! ({datetime.now().strftime('%H:%M:%S')})\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(check_interval)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\nëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨ë¨. ({datetime.now().strftime('%H:%M:%S')})\")\n",
    "    \n",
    "    return current_count\n",
    "\n",
    "print(\"ì‹¤í—˜ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ë ¤ë©´ monitor_experiments()ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì‹  ì‹¤í—˜ ê²°ê³¼ ì¦ì‹œ ë¡œë“œ ë° ë¶„ì„\n",
    "print(\"=== ìµœì‹  WADI ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ===\")\n",
    "\n",
    "# ìµœì‹  ê²°ê³¼ í´ë”ë“¤ ë‹¤ì‹œ ë¡œë“œ\n",
    "latest_folders = [f for f in os.listdir(result_base_path) \n",
    "                 if os.path.isdir(os.path.join(result_base_path, f)) and f != 'logs']\n",
    "latest_folders.sort(reverse=True)\n",
    "\n",
    "results_summary = []\n",
    "for i, folder in enumerate(latest_folders[:5]):  # ìµœì‹  5ê°œ\n",
    "    config_path = os.path.join(result_base_path, folder, 'config.txt')\n",
    "    summary_path = os.path.join(result_base_path, folder, 'summary.txt')\n",
    "    \n",
    "    if os.path.exists(config_path) and os.path.exists(summary_path):\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "            with open(summary_path, 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            \n",
    "            result = {\n",
    "                'folder': folder,\n",
    "                'comment': config.get('comment', 'N/A'),\n",
    "                'epochs': config.get('epochs', 1),\n",
    "                'tlcc_threshold': config.get('tlcc_threshold', 1.0),\n",
    "                'tlcc_binary': config.get('tlcc_binary', False),\n",
    "                'f1': summary['epsilon_result']['f1'],\n",
    "                'precision': summary['epsilon_result']['precision'],\n",
    "                'recall': summary['epsilon_result']['recall'],\n",
    "                'TP': summary['epsilon_result']['TP'],\n",
    "                'FP': summary['epsilon_result']['FP'],\n",
    "                'TN': summary['epsilon_result']['TN'],\n",
    "                'FN': summary['epsilon_result']['FN']\n",
    "            }\n",
    "            results_summary.append(result)\n",
    "            \n",
    "            print(f\"\\n{i+1}. {folder} ({config.get('comment', 'N/A')})\")\n",
    "            print(f\"   TLCC: {config.get('tlcc_threshold', 'N/A')}, Epochs: {config.get('epochs', 'N/A')}\")\n",
    "            print(f\"   F1: {summary['epsilon_result']['f1']:.4f}, Prec: {summary['epsilon_result']['precision']:.4f}, Rec: {summary['epsilon_result']['recall']:.4f}\")\n",
    "            print(f\"   TP: {summary['epsilon_result']['TP']}, FP: {summary['epsilon_result']['FP']}, FN: {summary['epsilon_result']['FN']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{i+1}. {folder} - ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "# TLCC thresholdë³„ ì„±ëŠ¥ ë¹„êµ\n",
    "if results_summary:\n",
    "    print(\"\\n=== TLCC Thresholdë³„ ì„±ëŠ¥ ë¹„êµ ===\")\n",
    "    tlcc_performance = {}\n",
    "    for result in results_summary:\n",
    "        tlcc = result['tlcc_threshold']\n",
    "        if tlcc not in tlcc_performance:\n",
    "            tlcc_performance[tlcc] = []\n",
    "        tlcc_performance[tlcc].append(result)\n",
    "    \n",
    "    for tlcc in sorted(tlcc_performance.keys(), reverse=True):\n",
    "        results = tlcc_performance[tlcc]\n",
    "        best_f1 = max(r['f1'] for r in results)\n",
    "        avg_f1 = sum(r['f1'] for r in results) / len(results)\n",
    "        print(f\"\\nTLCC {tlcc}:\")\n",
    "        print(f\"  ìµœê³  F1: {best_f1:.4f}\")\n",
    "        print(f\"  í‰ê·  F1: {avg_f1:.4f}\")\n",
    "        print(f\"  ì‹¤í—— ìˆ˜: {len(results)}ê°œ\")\n",
    "        \n",
    "        # ì—°ê²° ë°€ë„ ì •ë³´\n",
    "        if tlcc == 0.8:\n",
    "            print(f\"  ì—°ê²° ë°€ë„: 1.2% (80/6642)\")\n",
    "        elif tlcc == 0.6:\n",
    "            print(f\"  ì—°ê²° ë°€ë„: 3.9% (256/6642)\")\n",
    "        elif tlcc == 0.5:\n",
    "            print(f\"  ì—°ê²° ë°€ë„: 6.9% (460/6642)\")\n",
    "\n",
    "print(f\"\\nì´ {len(results_summary)}ê°œì˜ ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7309df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™•ì¥ëœ í‰ê°€ì§€í‘œ ë¶„ì„\n",
    "import sys\n",
    "sys.path.append('/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi')\n",
    "from eval_methods import calculate_additional_metrics\n",
    "\n",
    "print(\"=== í™•ì¥ëœ í‰ê°€ì§€í‘œ (ROC-AUC, PR-AUC, MCC) ë¶„ì„ ===\")\n",
    "\n",
    "# CSV íŒŒì¼ì—ì„œ í™•ì¥ëœ ì§€í‘œ ê²°ê³¼ ë¡œë“œ\n",
    "csv_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/extended_metrics_results.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    extended_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"í™•ì¥ëœ ì§€í‘œ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(extended_df)}ê°œ ì‹¤í—˜\")\n",
    "    \n",
    "    # ì£¼ìš” ì§€í‘œ ë¹„êµ\n",
    "    print(\"\\nğŸ“Š ì£¼ìš” ì§€í‘œ ë¹„êµ:\")\n",
    "    comparison_metrics = ['f1', 'roc_auc', 'pr_auc', 'mcc']\n",
    "    \n",
    "    for i, (idx, row) in enumerate(extended_df.iterrows()):\n",
    "        print(f\"\\n{i+1}. {row['comment'][:40]}...\")\n",
    "        print(f\"   TLCC: {row['tlcc_threshold']}, Epochs: {row['epochs']}\")\n",
    "        print(f\"   F1: {row['f1']:.4f} | ROC-AUC: {row['roc_auc']:.4f} | PR-AUC: {row['pr_auc']:.4f} | MCC: {row['mcc']:.4f}\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì§€í‘œë³„ ì¶”ì¶œ\n",
    "    print(\"\\n\\nğŸ† ì§€í‘œë³„ ìµœê³  ì„±ëŠ¥:\")\n",
    "    \n",
    "    best_f1 = extended_df.loc[extended_df['f1'].idxmax()]\n",
    "    best_roc = extended_df.loc[extended_df['roc_auc'].idxmax()]\n",
    "    best_pr = extended_df.loc[extended_df['pr_auc'].idxmax()]\n",
    "    best_mcc = extended_df.loc[extended_df['mcc'].idxmax()]\n",
    "    \n",
    "    print(f\"F1 Score ìµœê³ : {best_f1['f1']:.4f} ({best_f1['comment'][:30]}...)\")\n",
    "    print(f\"ROC-AUC ìµœê³ : {best_roc['roc_auc']:.4f} ({best_roc['comment'][:30]}...)\")\n",
    "    print(f\"PR-AUC ìµœê³ : {best_pr['pr_auc']:.4f} ({best_pr['comment'][:30]}...)\")\n",
    "    print(f\"MCC ìµœê³ : {best_mcc['mcc']:.4f} ({best_mcc['comment'][:30]}...)\")\n",
    "    \n",
    "    # TLCC thresholdë³„ í‰ê·  ì„±ëŠ¥\n",
    "    print(\"\\n\\nğŸ”— TLCC Thresholdë³„ í‰ê·  ì„±ëŠ¥:\")\n",
    "    tlcc_avg = extended_df.groupby('tlcc_threshold')[comparison_metrics].mean()\n",
    "    for threshold in tlcc_avg.index:\n",
    "        print(f\"\\nTLCC {threshold}:\")\n",
    "        print(f\"  F1: {tlcc_avg.loc[threshold, 'f1']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {tlcc_avg.loc[threshold, 'roc_auc']:.4f}\")\n",
    "        print(f\"  PR-AUC: {tlcc_avg.loc[threshold, 'pr_auc']:.4f}\")\n",
    "        print(f\"  MCC: {tlcc_avg.loc[threshold, 'mcc']:.4f}\")\n",
    "    \n",
    "    # ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "    print(\"\\n\\nğŸ” ì§€í‘œ ê°„ ìƒê´€ê´€ê³„:\")\n",
    "    corr_matrix = extended_df[comparison_metrics].corr()\n",
    "    print(corr_matrix.round(3))\n",
    "    \n",
    "else:\n",
    "    print(\"í™•ì¥ëœ ì§€í‘œ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"analyze_extended_metrics.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# í‰ê°€ì§€í‘œ ì„¤ëª…\n",
    "print(\"\\n\\nğŸ“š í‰ê°€ì§€í‘œ ì„¤ëª…:\")\n",
    "print(\"ğŸ… F1 Score: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê·  (0~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "print(\"ğŸ¨ ROC-AUC: ëª¨ë“  ì„ê³„ê°’ì—ì„œ TPR vs FPR ê³¡ì„  ì•„ë˜ ë©´ì  (0~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "print(\"ğŸ¯ PR-AUC: Precision-Recall ê³¡ì„  ì•„ë˜ ë©´ì , ë¶ˆê· í˜• ë°ì´í„°ì— ì í•© (0~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "print(\"âš–ï¸ MCC: Matthews Correlation Coefficient, ê°€ì¥ ì‹ ë¢°í•  ë§Œí•œ ì§€í‘œ (-1~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "print(\"\\nì´ìƒ íƒì§€ì—ì„œëŠ” PR-AUCì™€ MCCê°€ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3545da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\n",
      "Available keys in test_output: ['Forecast_0', 'True_0', 'A_Score_0', 'Forecast_1', 'True_1', 'A_Score_1', 'Forecast_2', 'True_2', 'A_Score_2', 'Forecast_3', 'True_3', 'A_Score_3', 'Forecast_4', 'True_4', 'A_Score_4', 'Forecast_5', 'True_5', 'A_Score_5', 'Forecast_6', 'True_6', 'A_Score_6', 'Forecast_7', 'True_7', 'A_Score_7', 'Forecast_8', 'True_8', 'A_Score_8', 'Forecast_9', 'True_9', 'A_Score_9', 'Forecast_10', 'True_10', 'A_Score_10', 'Forecast_11', 'True_11', 'A_Score_11', 'Forecast_12', 'True_12', 'A_Score_12', 'Forecast_13', 'True_13', 'A_Score_13', 'Forecast_14', 'True_14', 'A_Score_14', 'Forecast_15', 'True_15', 'A_Score_15', 'Forecast_16', 'True_16', 'A_Score_16', 'Forecast_17', 'True_17', 'A_Score_17', 'Forecast_18', 'True_18', 'A_Score_18', 'Forecast_19', 'True_19', 'A_Score_19', 'Forecast_20', 'True_20', 'A_Score_20', 'Forecast_21', 'True_21', 'A_Score_21', 'Forecast_22', 'True_22', 'A_Score_22', 'Forecast_23', 'True_23', 'A_Score_23', 'Forecast_24', 'True_24', 'A_Score_24', 'Forecast_25', 'True_25', 'A_Score_25', 'Forecast_26', 'True_26', 'A_Score_26', 'Forecast_27', 'True_27', 'A_Score_27', 'Forecast_28', 'True_28', 'A_Score_28', 'Forecast_29', 'True_29', 'A_Score_29', 'Forecast_30', 'True_30', 'A_Score_30', 'Forecast_31', 'True_31', 'A_Score_31', 'Forecast_32', 'True_32', 'A_Score_32', 'Forecast_33', 'True_33', 'A_Score_33', 'Forecast_34', 'True_34', 'A_Score_34', 'Forecast_35', 'True_35', 'A_Score_35', 'Forecast_36', 'True_36', 'A_Score_36', 'Forecast_37', 'True_37', 'A_Score_37', 'Forecast_38', 'True_38', 'A_Score_38', 'Forecast_39', 'True_39', 'A_Score_39', 'Forecast_40', 'True_40', 'A_Score_40', 'Forecast_41', 'True_41', 'A_Score_41', 'Forecast_42', 'True_42', 'A_Score_42', 'Forecast_43', 'True_43', 'A_Score_43', 'Forecast_44', 'True_44', 'A_Score_44', 'Forecast_45', 'True_45', 'A_Score_45', 'Forecast_46', 'True_46', 'A_Score_46', 'Forecast_47', 'True_47', 'A_Score_47', 'Forecast_48', 'True_48', 'A_Score_48', 'Forecast_49', 'True_49', 'A_Score_49', 'Forecast_50', 'True_50', 'A_Score_50', 'Forecast_51', 'True_51', 'A_Score_51', 'Forecast_52', 'True_52', 'A_Score_52', 'Forecast_53', 'True_53', 'A_Score_53', 'Forecast_54', 'True_54', 'A_Score_54', 'Forecast_55', 'True_55', 'A_Score_55', 'Forecast_56', 'True_56', 'A_Score_56', 'Forecast_57', 'True_57', 'A_Score_57', 'Forecast_58', 'True_58', 'A_Score_58', 'Forecast_59', 'True_59', 'A_Score_59', 'Forecast_60', 'True_60', 'A_Score_60', 'Forecast_61', 'True_61', 'A_Score_61', 'Forecast_62', 'True_62', 'A_Score_62', 'Forecast_63', 'True_63', 'A_Score_63', 'Forecast_64', 'True_64', 'A_Score_64', 'Forecast_65', 'True_65', 'A_Score_65', 'Forecast_66', 'True_66', 'A_Score_66', 'Forecast_67', 'True_67', 'A_Score_67', 'Forecast_68', 'True_68', 'A_Score_68', 'Forecast_69', 'True_69', 'A_Score_69', 'Forecast_70', 'True_70', 'A_Score_70', 'Forecast_71', 'True_71', 'A_Score_71', 'Forecast_72', 'True_72', 'A_Score_72', 'Forecast_73', 'True_73', 'A_Score_73', 'Forecast_74', 'True_74', 'A_Score_74', 'Forecast_75', 'True_75', 'A_Score_75', 'Forecast_76', 'True_76', 'A_Score_76', 'Forecast_77', 'True_77', 'A_Score_77', 'Forecast_78', 'True_78', 'A_Score_78', 'Forecast_79', 'True_79', 'A_Score_79', 'Forecast_80', 'True_80', 'A_Score_80', 'Forecast_81', 'True_81', 'A_Score_81', 'A_Score_Global', 'A_Pred_0', 'Thresh_0', 'A_Pred_1', 'Thresh_1', 'A_Pred_2', 'Thresh_2', 'A_Pred_3', 'Thresh_3', 'A_Pred_4', 'Thresh_4', 'A_Pred_5', 'Thresh_5', 'A_Pred_6', 'Thresh_6', 'A_Pred_7', 'Thresh_7', 'A_Pred_8', 'Thresh_8', 'A_Pred_9', 'Thresh_9', 'A_Pred_10', 'Thresh_10', 'A_Pred_11', 'Thresh_11', 'A_Pred_12', 'Thresh_12', 'A_Pred_13', 'Thresh_13', 'A_Pred_14', 'Thresh_14', 'A_Pred_15', 'Thresh_15', 'A_Pred_16', 'Thresh_16', 'A_Pred_17', 'Thresh_17', 'A_Pred_18', 'Thresh_18', 'A_Pred_19', 'Thresh_19', 'A_Pred_20', 'Thresh_20', 'A_Pred_21', 'Thresh_21', 'A_Pred_22', 'Thresh_22', 'A_Pred_23', 'Thresh_23', 'A_Pred_24', 'Thresh_24', 'A_Pred_25', 'Thresh_25', 'A_Pred_26', 'Thresh_26', 'A_Pred_27', 'Thresh_27', 'A_Pred_28', 'Thresh_28', 'A_Pred_29', 'Thresh_29', 'A_Pred_30', 'Thresh_30', 'A_Pred_31', 'Thresh_31', 'A_Pred_32', 'Thresh_32', 'A_Pred_33', 'Thresh_33', 'A_Pred_34', 'Thresh_34', 'A_Pred_35', 'Thresh_35', 'A_Pred_36', 'Thresh_36', 'A_Pred_37', 'Thresh_37', 'A_Pred_38', 'Thresh_38', 'A_Pred_39', 'Thresh_39', 'A_Pred_40', 'Thresh_40', 'A_Pred_41', 'Thresh_41', 'A_Pred_42', 'Thresh_42', 'A_Pred_43', 'Thresh_43', 'A_Pred_44', 'Thresh_44', 'A_Pred_45', 'Thresh_45', 'A_Pred_46', 'Thresh_46', 'A_Pred_47', 'Thresh_47', 'A_Pred_48', 'Thresh_48', 'A_Pred_49', 'Thresh_49', 'A_Pred_50', 'Thresh_50', 'A_Pred_51', 'Thresh_51', 'A_Pred_52', 'Thresh_52', 'A_Pred_53', 'Thresh_53', 'A_Pred_54', 'Thresh_54', 'A_Pred_55', 'Thresh_55', 'A_Pred_56', 'Thresh_56', 'A_Pred_57', 'Thresh_57', 'A_Pred_58', 'Thresh_58', 'A_Pred_59', 'Thresh_59', 'A_Pred_60', 'Thresh_60', 'A_Pred_61', 'Thresh_61', 'A_Pred_62', 'Thresh_62', 'A_Pred_63', 'Thresh_63', 'A_Pred_64', 'Thresh_64', 'A_Pred_65', 'Thresh_65', 'A_Pred_66', 'Thresh_66', 'A_Pred_67', 'Thresh_67', 'A_Pred_68', 'Thresh_68', 'A_Pred_69', 'Thresh_69', 'A_Pred_70', 'Thresh_70', 'A_Pred_71', 'Thresh_71', 'A_Pred_72', 'Thresh_72', 'A_Pred_73', 'Thresh_73', 'A_Pred_74', 'Thresh_74', 'A_Pred_75', 'Thresh_75', 'A_Pred_76', 'Thresh_76', 'A_Pred_77', 'Thresh_77', 'A_Pred_78', 'Thresh_78', 'A_Pred_79', 'Thresh_79', 'A_Pred_80', 'Thresh_80', 'A_Pred_81', 'Thresh_81', 'A_True_Global', 'Thresh_Global', 'A_Pred_Global']\n",
      "test_outputì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì„ê³„ê°’ ìµœì í™” ë¶„ì„\n",
    "print(\"\\n=== ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\")\n",
    "\n",
    "# test_outputì—ì„œ anomaly scoresì™€ labels ì¶”ì¶œ\n",
    "print(\"Available keys in test_output:\", list(test_output.keys()))\n",
    "\n",
    "# scoresì™€ labels í™•ì¸\n",
    "if 'test_scores' in test_output and 'test_labels' in test_output:\n",
    "    scores = test_output['test_scores']\n",
    "    labels = test_output['test_labels']\n",
    "    \n",
    "    print(f\"\\nAnomaly scores í†µê³„:\")\n",
    "    print(f\"- Min: {np.min(scores):.6f}\")\n",
    "    print(f\"- Max: {np.max(scores):.6f}\")\n",
    "    print(f\"- Mean: {np.mean(scores):.6f}\")\n",
    "    print(f\"- Std: {np.std(scores):.6f}\")\n",
    "    print(f\"- í˜„ì¬ threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nLabels í†µê³„:\")\n",
    "    print(f\"- Total samples: {len(labels)}\")\n",
    "    print(f\"- Normal samples: {np.sum(labels == 0)}\")\n",
    "    print(f\"- Anomaly samples: {np.sum(labels == 1)}\")\n",
    "    print(f\"- Anomaly ratio: {np.sum(labels == 1) / len(labels):.4f}\")\n",
    "    \n",
    "    # Precision-Recall ê³¡ì„ ì„ ì´ìš©í•œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    \n",
    "    # F1 score ê³„ì‚°\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # ìµœì  F1 score ì°¾ê¸°\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else thresholds[-1]\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_precision = precision[best_f1_idx]\n",
    "    best_recall = recall[best_f1_idx]\n",
    "    \n",
    "    print(f\"\\n=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  F1 score: {best_f1:.4f}\")\n",
    "    print(f\"- ìµœì  Precision: {best_precision:.4f}\")\n",
    "    print(f\"- ìµœì  Recall: {best_recall:.4f}\")\n",
    "    \n",
    "    # í˜„ì¬ thresholdì™€ ë¹„êµ\n",
    "    current_threshold = summary['epsilon_result']['threshold']\n",
    "    print(f\"\\n=== í˜„ì¬ vs ìµœì  ë¹„êµ ===\")\n",
    "    print(f\"- í˜„ì¬ threshold: {current_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- Threshold ë¹„ìœ¨: {best_threshold/current_threshold:.2f}x\")\n",
    "    \n",
    "else:\n",
    "    print(\"test_outputì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6f0044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì˜¬ë°”ë¥¸ ë°ì´í„°ë¡œ ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\n",
      "\n",
      "Anomaly scores í†µê³„:\n",
      "- Min: 0.026701\n",
      "- Max: 0.387734\n",
      "- Mean: 0.071146\n",
      "- Median: 0.069723\n",
      "- Std: 0.023162\n",
      "- 90%ile: 0.100857\n",
      "- 95%ile: 0.108942\n",
      "- 99%ile: 0.133822\n",
      "- í˜„ì¬ threshold: 0.070347\n",
      "\n",
      "Labels í†µê³„:\n",
      "- Total samples: 51742\n",
      "- Normal samples: 48603\n",
      "- Anomaly samples: 3139\n",
      "- Anomaly ratio: 0.0607\n",
      "\n",
      "=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\n",
      "- ìµœì  threshold: 0.093159\n",
      "- ìµœì  F1 score: 0.2782\n",
      "- ìµœì  Precision: 0.1857\n",
      "- ìµœì  Recall: 0.5540\n",
      "\n",
      "=== í˜„ì¬ vs ìµœì  ë¹„êµ ===\n",
      "- í˜„ì¬ threshold: 0.070347\n",
      "- ìµœì  threshold: 0.093159\n",
      "- Threshold ë¹„ìœ¨: 1.32x\n",
      "\n",
      "=== ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ===\n",
      "í˜„ì¬ threshold (0.070347): F1=0.1973, P=0.1108, R=0.8968 [TP:2815, FP:22584, FN:324]\n",
      "ìµœì  threshold (0.093159): F1=0.2780, P=0.1856, R=0.5537 [TP:1738, FP:7625, FN:1401]\n",
      "90%ile threshold (0.100857): F1=0.2155, P=0.1731, R=0.2854 [TP:896, FP:4279, FN:2243]\n",
      "95%ile threshold (0.108942): F1=0.1970, P=0.2179, R=0.1797 [TP:564, FP:2024, FN:2575]\n",
      "99%ile threshold (0.133822): F1=0.0104, P=0.0367, R=0.0061 [TP:19, FP:499, FN:3120]\n",
      "99.5%ile threshold (0.151068): F1=0.0035, P=0.0232, R=0.0019 [TP:6, FP:253, FN:3133]\n",
      "99.9%ile threshold (0.176695): F1=0.0000, P=0.0000, R=0.0000 [TP:0, FP:52, FN:3139]\n"
     ]
    }
   ],
   "source": [
    "# ì˜¬ë°”ë¥¸ ë°ì´í„° í‚¤ë¡œ ì„ê³„ê°’ ìµœì í™” ë¶„ì„\n",
    "print(\"\\n=== ì˜¬ë°”ë¥¸ ë°ì´í„°ë¡œ ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\")\n",
    "\n",
    "# Global anomaly scoresì™€ labels ì¶”ì¶œ\n",
    "if 'A_Score_Global' in test_output and 'A_True_Global' in test_output:\n",
    "    scores = test_output['A_Score_Global']\n",
    "    labels = test_output['A_True_Global']\n",
    "    \n",
    "    print(f\"\\nAnomaly scores í†µê³„:\")\n",
    "    print(f\"- Min: {np.min(scores):.6f}\")\n",
    "    print(f\"- Max: {np.max(scores):.6f}\")\n",
    "    print(f\"- Mean: {np.mean(scores):.6f}\")\n",
    "    print(f\"- Median: {np.median(scores):.6f}\")\n",
    "    print(f\"- Std: {np.std(scores):.6f}\")\n",
    "    print(f\"- 90%ile: {np.percentile(scores, 90):.6f}\")\n",
    "    print(f\"- 95%ile: {np.percentile(scores, 95):.6f}\")\n",
    "    print(f\"- 99%ile: {np.percentile(scores, 99):.6f}\")\n",
    "    print(f\"- í˜„ì¬ threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nLabels í†µê³„:\")\n",
    "    print(f\"- Total samples: {len(labels)}\")\n",
    "    print(f\"- Normal samples: {np.sum(labels == 0)}\")\n",
    "    print(f\"- Anomaly samples: {np.sum(labels == 1)}\")\n",
    "    print(f\"- Anomaly ratio: {np.sum(labels == 1) / len(labels):.4f}\")\n",
    "    \n",
    "    # Precision-Recall ê³¡ì„ ì„ ì´ìš©í•œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    \n",
    "    # F1 score ê³„ì‚°\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # ìµœì  F1 score ì°¾ê¸°\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else thresholds[-1]\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_precision = precision[best_f1_idx]\n",
    "    best_recall = recall[best_f1_idx]\n",
    "    \n",
    "    print(f\"\\n=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  F1 score: {best_f1:.4f}\")\n",
    "    print(f\"- ìµœì  Precision: {best_precision:.4f}\")\n",
    "    print(f\"- ìµœì  Recall: {best_recall:.4f}\")\n",
    "    \n",
    "    # í˜„ì¬ thresholdì™€ ë¹„êµ\n",
    "    current_threshold = summary['epsilon_result']['threshold']\n",
    "    print(f\"\\n=== í˜„ì¬ vs ìµœì  ë¹„êµ ===\")\n",
    "    print(f\"- í˜„ì¬ threshold: {current_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- Threshold ë¹„ìœ¨: {best_threshold/current_threshold:.2f}x\")\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ í™•ì¸\n",
    "    print(f\"\\n=== ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ===\")\n",
    "    test_thresholds = [\n",
    "        current_threshold, \n",
    "        best_threshold,\n",
    "        np.percentile(scores, 90), \n",
    "        np.percentile(scores, 95), \n",
    "        np.percentile(scores, 99),\n",
    "        np.percentile(scores, 99.5),\n",
    "        np.percentile(scores, 99.9)\n",
    "    ]\n",
    "    \n",
    "    threshold_names = [\n",
    "        'í˜„ì¬',\n",
    "        'ìµœì ',\n",
    "        '90%ile',\n",
    "        '95%ile', \n",
    "        '99%ile',\n",
    "        '99.5%ile',\n",
    "        '99.9%ile'\n",
    "    ]\n",
    "    \n",
    "    for i, (th, name) in enumerate(zip(test_thresholds, threshold_names)):\n",
    "        pred = (scores > th).astype(int)\n",
    "        f1 = f1_score(labels, pred)\n",
    "        prec = precision_score(labels, pred, zero_division=0)\n",
    "        rec = recall_score(labels, pred, zero_division=0)\n",
    "        \n",
    "        # TP, FP, TN, FN ê³„ì‚°\n",
    "        tp = np.sum((pred == 1) & (labels == 1))\n",
    "        fp = np.sum((pred == 1) & (labels == 0))\n",
    "        tn = np.sum((pred == 0) & (labels == 0))\n",
    "        fn = np.sum((pred == 0) & (labels == 1))\n",
    "        \n",
    "        print(f\"{name} threshold ({th:.6f}): F1={f1:.4f}, P={prec:.4f}, R={rec:.4f} [TP:{tp}, FP:{fp}, FN:{fn}]\")\n",
    "        \n",
    "else:\n",
    "    print(\"Global anomaly scoresë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3bb429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\n",
      "Available keys in test_output: ['Forecast_0', 'True_0', 'A_Score_0', 'Forecast_1', 'True_1', 'A_Score_1', 'Forecast_2', 'True_2', 'A_Score_2', 'Forecast_3', 'True_3', 'A_Score_3', 'Forecast_4', 'True_4', 'A_Score_4', 'Forecast_5', 'True_5', 'A_Score_5', 'Forecast_6', 'True_6', 'A_Score_6', 'Forecast_7', 'True_7', 'A_Score_7', 'Forecast_8', 'True_8', 'A_Score_8', 'Forecast_9', 'True_9', 'A_Score_9', 'Forecast_10', 'True_10', 'A_Score_10', 'Forecast_11', 'True_11', 'A_Score_11', 'Forecast_12', 'True_12', 'A_Score_12', 'Forecast_13', 'True_13', 'A_Score_13', 'Forecast_14', 'True_14', 'A_Score_14', 'Forecast_15', 'True_15', 'A_Score_15', 'Forecast_16', 'True_16', 'A_Score_16', 'Forecast_17', 'True_17', 'A_Score_17', 'Forecast_18', 'True_18', 'A_Score_18', 'Forecast_19', 'True_19', 'A_Score_19', 'Forecast_20', 'True_20', 'A_Score_20', 'Forecast_21', 'True_21', 'A_Score_21', 'Forecast_22', 'True_22', 'A_Score_22', 'Forecast_23', 'True_23', 'A_Score_23', 'Forecast_24', 'True_24', 'A_Score_24', 'Forecast_25', 'True_25', 'A_Score_25', 'Forecast_26', 'True_26', 'A_Score_26', 'Forecast_27', 'True_27', 'A_Score_27', 'Forecast_28', 'True_28', 'A_Score_28', 'Forecast_29', 'True_29', 'A_Score_29', 'Forecast_30', 'True_30', 'A_Score_30', 'Forecast_31', 'True_31', 'A_Score_31', 'Forecast_32', 'True_32', 'A_Score_32', 'Forecast_33', 'True_33', 'A_Score_33', 'Forecast_34', 'True_34', 'A_Score_34', 'Forecast_35', 'True_35', 'A_Score_35', 'Forecast_36', 'True_36', 'A_Score_36', 'Forecast_37', 'True_37', 'A_Score_37', 'Forecast_38', 'True_38', 'A_Score_38', 'Forecast_39', 'True_39', 'A_Score_39', 'Forecast_40', 'True_40', 'A_Score_40', 'Forecast_41', 'True_41', 'A_Score_41', 'Forecast_42', 'True_42', 'A_Score_42', 'Forecast_43', 'True_43', 'A_Score_43', 'Forecast_44', 'True_44', 'A_Score_44', 'Forecast_45', 'True_45', 'A_Score_45', 'Forecast_46', 'True_46', 'A_Score_46', 'Forecast_47', 'True_47', 'A_Score_47', 'Forecast_48', 'True_48', 'A_Score_48', 'Forecast_49', 'True_49', 'A_Score_49', 'Forecast_50', 'True_50', 'A_Score_50', 'Forecast_51', 'True_51', 'A_Score_51', 'Forecast_52', 'True_52', 'A_Score_52', 'Forecast_53', 'True_53', 'A_Score_53', 'Forecast_54', 'True_54', 'A_Score_54', 'Forecast_55', 'True_55', 'A_Score_55', 'Forecast_56', 'True_56', 'A_Score_56', 'Forecast_57', 'True_57', 'A_Score_57', 'Forecast_58', 'True_58', 'A_Score_58', 'Forecast_59', 'True_59', 'A_Score_59', 'Forecast_60', 'True_60', 'A_Score_60', 'Forecast_61', 'True_61', 'A_Score_61', 'Forecast_62', 'True_62', 'A_Score_62', 'Forecast_63', 'True_63', 'A_Score_63', 'Forecast_64', 'True_64', 'A_Score_64', 'Forecast_65', 'True_65', 'A_Score_65', 'Forecast_66', 'True_66', 'A_Score_66', 'Forecast_67', 'True_67', 'A_Score_67', 'Forecast_68', 'True_68', 'A_Score_68', 'Forecast_69', 'True_69', 'A_Score_69', 'Forecast_70', 'True_70', 'A_Score_70', 'Forecast_71', 'True_71', 'A_Score_71', 'Forecast_72', 'True_72', 'A_Score_72', 'Forecast_73', 'True_73', 'A_Score_73', 'Forecast_74', 'True_74', 'A_Score_74', 'Forecast_75', 'True_75', 'A_Score_75', 'Forecast_76', 'True_76', 'A_Score_76', 'Forecast_77', 'True_77', 'A_Score_77', 'Forecast_78', 'True_78', 'A_Score_78', 'Forecast_79', 'True_79', 'A_Score_79', 'Forecast_80', 'True_80', 'A_Score_80', 'Forecast_81', 'True_81', 'A_Score_81', 'A_Score_Global', 'A_Pred_0', 'Thresh_0', 'A_Pred_1', 'Thresh_1', 'A_Pred_2', 'Thresh_2', 'A_Pred_3', 'Thresh_3', 'A_Pred_4', 'Thresh_4', 'A_Pred_5', 'Thresh_5', 'A_Pred_6', 'Thresh_6', 'A_Pred_7', 'Thresh_7', 'A_Pred_8', 'Thresh_8', 'A_Pred_9', 'Thresh_9', 'A_Pred_10', 'Thresh_10', 'A_Pred_11', 'Thresh_11', 'A_Pred_12', 'Thresh_12', 'A_Pred_13', 'Thresh_13', 'A_Pred_14', 'Thresh_14', 'A_Pred_15', 'Thresh_15', 'A_Pred_16', 'Thresh_16', 'A_Pred_17', 'Thresh_17', 'A_Pred_18', 'Thresh_18', 'A_Pred_19', 'Thresh_19', 'A_Pred_20', 'Thresh_20', 'A_Pred_21', 'Thresh_21', 'A_Pred_22', 'Thresh_22', 'A_Pred_23', 'Thresh_23', 'A_Pred_24', 'Thresh_24', 'A_Pred_25', 'Thresh_25', 'A_Pred_26', 'Thresh_26', 'A_Pred_27', 'Thresh_27', 'A_Pred_28', 'Thresh_28', 'A_Pred_29', 'Thresh_29', 'A_Pred_30', 'Thresh_30', 'A_Pred_31', 'Thresh_31', 'A_Pred_32', 'Thresh_32', 'A_Pred_33', 'Thresh_33', 'A_Pred_34', 'Thresh_34', 'A_Pred_35', 'Thresh_35', 'A_Pred_36', 'Thresh_36', 'A_Pred_37', 'Thresh_37', 'A_Pred_38', 'Thresh_38', 'A_Pred_39', 'Thresh_39', 'A_Pred_40', 'Thresh_40', 'A_Pred_41', 'Thresh_41', 'A_Pred_42', 'Thresh_42', 'A_Pred_43', 'Thresh_43', 'A_Pred_44', 'Thresh_44', 'A_Pred_45', 'Thresh_45', 'A_Pred_46', 'Thresh_46', 'A_Pred_47', 'Thresh_47', 'A_Pred_48', 'Thresh_48', 'A_Pred_49', 'Thresh_49', 'A_Pred_50', 'Thresh_50', 'A_Pred_51', 'Thresh_51', 'A_Pred_52', 'Thresh_52', 'A_Pred_53', 'Thresh_53', 'A_Pred_54', 'Thresh_54', 'A_Pred_55', 'Thresh_55', 'A_Pred_56', 'Thresh_56', 'A_Pred_57', 'Thresh_57', 'A_Pred_58', 'Thresh_58', 'A_Pred_59', 'Thresh_59', 'A_Pred_60', 'Thresh_60', 'A_Pred_61', 'Thresh_61', 'A_Pred_62', 'Thresh_62', 'A_Pred_63', 'Thresh_63', 'A_Pred_64', 'Thresh_64', 'A_Pred_65', 'Thresh_65', 'A_Pred_66', 'Thresh_66', 'A_Pred_67', 'Thresh_67', 'A_Pred_68', 'Thresh_68', 'A_Pred_69', 'Thresh_69', 'A_Pred_70', 'Thresh_70', 'A_Pred_71', 'Thresh_71', 'A_Pred_72', 'Thresh_72', 'A_Pred_73', 'Thresh_73', 'A_Pred_74', 'Thresh_74', 'A_Pred_75', 'Thresh_75', 'A_Pred_76', 'Thresh_76', 'A_Pred_77', 'Thresh_77', 'A_Pred_78', 'Thresh_78', 'A_Pred_79', 'Thresh_79', 'A_Pred_80', 'Thresh_80', 'A_Pred_81', 'Thresh_81', 'A_True_Global', 'Thresh_Global', 'A_Pred_Global']\n",
      "Available keys for labels: []\n",
      "test_outputì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "Available keys: ['Forecast_0', 'True_0', 'A_Score_0', 'Forecast_1', 'True_1', 'A_Score_1', 'Forecast_2', 'True_2', 'A_Score_2', 'Forecast_3', 'True_3', 'A_Score_3', 'Forecast_4', 'True_4', 'A_Score_4', 'Forecast_5', 'True_5', 'A_Score_5', 'Forecast_6', 'True_6', 'A_Score_6', 'Forecast_7', 'True_7', 'A_Score_7', 'Forecast_8', 'True_8', 'A_Score_8', 'Forecast_9', 'True_9', 'A_Score_9', 'Forecast_10', 'True_10', 'A_Score_10', 'Forecast_11', 'True_11', 'A_Score_11', 'Forecast_12', 'True_12', 'A_Score_12', 'Forecast_13', 'True_13', 'A_Score_13', 'Forecast_14', 'True_14', 'A_Score_14', 'Forecast_15', 'True_15', 'A_Score_15', 'Forecast_16', 'True_16', 'A_Score_16', 'Forecast_17', 'True_17', 'A_Score_17', 'Forecast_18', 'True_18', 'A_Score_18', 'Forecast_19', 'True_19', 'A_Score_19', 'Forecast_20', 'True_20', 'A_Score_20', 'Forecast_21', 'True_21', 'A_Score_21', 'Forecast_22', 'True_22', 'A_Score_22', 'Forecast_23', 'True_23', 'A_Score_23', 'Forecast_24', 'True_24', 'A_Score_24', 'Forecast_25', 'True_25', 'A_Score_25', 'Forecast_26', 'True_26', 'A_Score_26', 'Forecast_27', 'True_27', 'A_Score_27', 'Forecast_28', 'True_28', 'A_Score_28', 'Forecast_29', 'True_29', 'A_Score_29', 'Forecast_30', 'True_30', 'A_Score_30', 'Forecast_31', 'True_31', 'A_Score_31', 'Forecast_32', 'True_32', 'A_Score_32', 'Forecast_33', 'True_33', 'A_Score_33', 'Forecast_34', 'True_34', 'A_Score_34', 'Forecast_35', 'True_35', 'A_Score_35', 'Forecast_36', 'True_36', 'A_Score_36', 'Forecast_37', 'True_37', 'A_Score_37', 'Forecast_38', 'True_38', 'A_Score_38', 'Forecast_39', 'True_39', 'A_Score_39', 'Forecast_40', 'True_40', 'A_Score_40', 'Forecast_41', 'True_41', 'A_Score_41', 'Forecast_42', 'True_42', 'A_Score_42', 'Forecast_43', 'True_43', 'A_Score_43', 'Forecast_44', 'True_44', 'A_Score_44', 'Forecast_45', 'True_45', 'A_Score_45', 'Forecast_46', 'True_46', 'A_Score_46', 'Forecast_47', 'True_47', 'A_Score_47', 'Forecast_48', 'True_48', 'A_Score_48', 'Forecast_49', 'True_49', 'A_Score_49', 'Forecast_50', 'True_50', 'A_Score_50', 'Forecast_51', 'True_51', 'A_Score_51', 'Forecast_52', 'True_52', 'A_Score_52', 'Forecast_53', 'True_53', 'A_Score_53', 'Forecast_54', 'True_54', 'A_Score_54', 'Forecast_55', 'True_55', 'A_Score_55', 'Forecast_56', 'True_56', 'A_Score_56', 'Forecast_57', 'True_57', 'A_Score_57', 'Forecast_58', 'True_58', 'A_Score_58', 'Forecast_59', 'True_59', 'A_Score_59', 'Forecast_60', 'True_60', 'A_Score_60', 'Forecast_61', 'True_61', 'A_Score_61', 'Forecast_62', 'True_62', 'A_Score_62', 'Forecast_63', 'True_63', 'A_Score_63', 'Forecast_64', 'True_64', 'A_Score_64', 'Forecast_65', 'True_65', 'A_Score_65', 'Forecast_66', 'True_66', 'A_Score_66', 'Forecast_67', 'True_67', 'A_Score_67', 'Forecast_68', 'True_68', 'A_Score_68', 'Forecast_69', 'True_69', 'A_Score_69', 'Forecast_70', 'True_70', 'A_Score_70', 'Forecast_71', 'True_71', 'A_Score_71', 'Forecast_72', 'True_72', 'A_Score_72', 'Forecast_73', 'True_73', 'A_Score_73', 'Forecast_74', 'True_74', 'A_Score_74', 'Forecast_75', 'True_75', 'A_Score_75', 'Forecast_76', 'True_76', 'A_Score_76', 'Forecast_77', 'True_77', 'A_Score_77', 'Forecast_78', 'True_78', 'A_Score_78', 'Forecast_79', 'True_79', 'A_Score_79', 'Forecast_80', 'True_80', 'A_Score_80', 'Forecast_81', 'True_81', 'A_Score_81', 'A_Score_Global', 'A_Pred_0', 'Thresh_0', 'A_Pred_1', 'Thresh_1', 'A_Pred_2', 'Thresh_2', 'A_Pred_3', 'Thresh_3', 'A_Pred_4', 'Thresh_4', 'A_Pred_5', 'Thresh_5', 'A_Pred_6', 'Thresh_6', 'A_Pred_7', 'Thresh_7', 'A_Pred_8', 'Thresh_8', 'A_Pred_9', 'Thresh_9', 'A_Pred_10', 'Thresh_10', 'A_Pred_11', 'Thresh_11', 'A_Pred_12', 'Thresh_12', 'A_Pred_13', 'Thresh_13', 'A_Pred_14', 'Thresh_14', 'A_Pred_15', 'Thresh_15', 'A_Pred_16', 'Thresh_16', 'A_Pred_17', 'Thresh_17', 'A_Pred_18', 'Thresh_18', 'A_Pred_19', 'Thresh_19', 'A_Pred_20', 'Thresh_20', 'A_Pred_21', 'Thresh_21', 'A_Pred_22', 'Thresh_22', 'A_Pred_23', 'Thresh_23', 'A_Pred_24', 'Thresh_24', 'A_Pred_25', 'Thresh_25', 'A_Pred_26', 'Thresh_26', 'A_Pred_27', 'Thresh_27', 'A_Pred_28', 'Thresh_28', 'A_Pred_29', 'Thresh_29', 'A_Pred_30', 'Thresh_30', 'A_Pred_31', 'Thresh_31', 'A_Pred_32', 'Thresh_32', 'A_Pred_33', 'Thresh_33', 'A_Pred_34', 'Thresh_34', 'A_Pred_35', 'Thresh_35', 'A_Pred_36', 'Thresh_36', 'A_Pred_37', 'Thresh_37', 'A_Pred_38', 'Thresh_38', 'A_Pred_39', 'Thresh_39', 'A_Pred_40', 'Thresh_40', 'A_Pred_41', 'Thresh_41', 'A_Pred_42', 'Thresh_42', 'A_Pred_43', 'Thresh_43', 'A_Pred_44', 'Thresh_44', 'A_Pred_45', 'Thresh_45', 'A_Pred_46', 'Thresh_46', 'A_Pred_47', 'Thresh_47', 'A_Pred_48', 'Thresh_48', 'A_Pred_49', 'Thresh_49', 'A_Pred_50', 'Thresh_50', 'A_Pred_51', 'Thresh_51', 'A_Pred_52', 'Thresh_52', 'A_Pred_53', 'Thresh_53', 'A_Pred_54', 'Thresh_54', 'A_Pred_55', 'Thresh_55', 'A_Pred_56', 'Thresh_56', 'A_Pred_57', 'Thresh_57', 'A_Pred_58', 'Thresh_58', 'A_Pred_59', 'Thresh_59', 'A_Pred_60', 'Thresh_60', 'A_Pred_61', 'Thresh_61', 'A_Pred_62', 'Thresh_62', 'A_Pred_63', 'Thresh_63', 'A_Pred_64', 'Thresh_64', 'A_Pred_65', 'Thresh_65', 'A_Pred_66', 'Thresh_66', 'A_Pred_67', 'Thresh_67', 'A_Pred_68', 'Thresh_68', 'A_Pred_69', 'Thresh_69', 'A_Pred_70', 'Thresh_70', 'A_Pred_71', 'Thresh_71', 'A_Pred_72', 'Thresh_72', 'A_Pred_73', 'Thresh_73', 'A_Pred_74', 'Thresh_74', 'A_Pred_75', 'Thresh_75', 'A_Pred_76', 'Thresh_76', 'A_Pred_77', 'Thresh_77', 'A_Pred_78', 'Thresh_78', 'A_Pred_79', 'Thresh_79', 'A_Pred_80', 'Thresh_80', 'A_Pred_81', 'Thresh_81', 'A_True_Global', 'Thresh_Global', 'A_Pred_Global']\n"
     ]
    }
   ],
   "source": [
    "# ì„ê³„ê°’ ìµœì í™” ë¶„ì„\n",
    "print(\"\\n=== ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\")\n",
    "\n",
    "# test_outputì—ì„œ anomaly scoresì™€ labels ì¶”ì¶œ\n",
    "if 'test_scores' in test_output:\n",
    "    anomaly_scores = test_output['test_scores']\n",
    "else:\n",
    "    # ë‹¤ë¥¸ í‚¤ ì´ë¦„ì¼ ìˆ˜ ìˆìŒ\n",
    "    print(\"Available keys in test_output:\", list(test_output.keys()))\n",
    "    \n",
    "# labels í™•ì¸\n",
    "if 'test_labels' in test_output:\n",
    "    true_labels = test_output['test_labels']\n",
    "else:\n",
    "    print(\"Available keys for labels:\", [k for k in test_output.keys() if 'label' in k.lower()])\n",
    "\n",
    "# ê¸°ë³¸ í†µê³„ í™•ì¸\n",
    "if 'test_scores' in test_output and 'test_labels' in test_output:\n",
    "    scores = test_output['test_scores']\n",
    "    labels = test_output['test_labels']\n",
    "    \n",
    "    print(f\"\\nAnomaly scores í†µê³„:\")\n",
    "    print(f\"- Min: {np.min(scores):.6f}\")\n",
    "    print(f\"- Max: {np.max(scores):.6f}\")\n",
    "    print(f\"- Mean: {np.mean(scores):.6f}\")\n",
    "    print(f\"- Std: {np.std(scores):.6f}\")\n",
    "    print(f\"- í˜„ì¬ threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "    \n",
    "    print(f\"\\nLabels í†µê³„:\")\n",
    "    print(f\"- Total samples: {len(labels)}\")\n",
    "    print(f\"- Normal samples: {np.sum(labels == 0)}\")\n",
    "    print(f\"- Anomaly samples: {np.sum(labels == 1)}\")\n",
    "    print(f\"- Anomaly ratio: {np.sum(labels == 1) / len(labels):.4f}\")\n",
    "    \n",
    "    # Precision-Recall ê³¡ì„ ì„ ì´ìš©í•œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    \n",
    "    # F1 score ê³„ì‚°\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # ìµœì  F1 score ì°¾ê¸°\n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else thresholds[-1]\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_precision = precision[best_f1_idx]\n",
    "    best_recall = recall[best_f1_idx]\n",
    "    \n",
    "    print(f\"\\n=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  F1 score: {best_f1:.4f}\")\n",
    "    print(f\"- ìµœì  Precision: {best_precision:.4f}\")\n",
    "    print(f\"- ìµœì  Recall: {best_recall:.4f}\")\n",
    "    \n",
    "    # í˜„ì¬ thresholdì™€ ë¹„êµ\n",
    "    current_threshold = summary['epsilon_result']['threshold']\n",
    "    print(f\"\\n=== í˜„ì¬ vs ìµœì  ë¹„êµ ===\")\n",
    "    print(f\"- í˜„ì¬ threshold: {current_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- Threshold ë¹„ìœ¨: {best_threshold/current_threshold:.2f}x\")\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ í™•ì¸\n",
    "    print(f\"\\n=== ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ===\")\n",
    "    test_thresholds = [current_threshold, best_threshold, \n",
    "                      np.percentile(scores, 90), np.percentile(scores, 95), \n",
    "                      np.percentile(scores, 99)]\n",
    "    \n",
    "    for i, th in enumerate(test_thresholds):\n",
    "        pred = (scores > th).astype(int)\n",
    "        f1 = f1_score(labels, pred)\n",
    "        prec = precision_score(labels, pred)\n",
    "        rec = recall_score(labels, pred)\n",
    "        \n",
    "        if i == 0:\n",
    "            print(f\"í˜„ì¬ threshold ({th:.6f}): F1={f1:.4f}, P={prec:.4f}, R={rec:.4f}\")\n",
    "        elif i == 1:\n",
    "            print(f\"ìµœì  threshold ({th:.6f}): F1={f1:.4f}, P={prec:.4f}, R={rec:.4f}\")\n",
    "        else:\n",
    "            percentile = [90, 95, 99][i-2]\n",
    "            print(f\"{percentile}% threshold ({th:.6f}): F1={f1:.4f}, P={prec:.4f}, R={rec:.4f}\")\n",
    "else:\n",
    "    print(\"test_outputì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"Available keys:\", list(test_output.keys()) if 'test_output' in locals() else \"test_output not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d4cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0916d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìµœì í™” ë°©ì•ˆ ===\n",
      "1. í˜„ì¬ threshold 0.070347ì—ì„œ\n",
      "   ìµœì  threshold 0.093159ë¡œ ë³€ê²½ í•„ìš”\n",
      "2. ì˜ˆìƒ ê°œì„  ì •ë„:\n",
      "   - F1 score: 0.2175 â†’ 0.2782 (+27.9%)\n",
      "   - Precision: 0.1220 â†’ 0.1857 (+52.2%)\n",
      "   - False Positives: 22584 â†’ 7625 (-66.2%)\n",
      "\n",
      "=== ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ ===\n",
      "1. ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ ì¬í•™ìŠµ ë˜ëŠ” ì„ê³„ê°’ ì¡°ì •\n",
      "2. POT(Peaks-Over-Threshold) ë°©ë²• ëŒ€ì‹  ê³ ì • ì„ê³„ê°’ ì‚¬ìš©\n",
      "3. TLCC threshold ì¡°ì • (1.0 ëŒ€ì‹  0.5~0.8 ì‹œë„)\n",
      "4. ë‹¤ë¥¸ ì´ìƒ íƒì§€ ì„ê³„ê°’ ë°©ë²• ì‹œë„\n",
      "\n",
      "ì¶”ì²œ ëª…ë ¹ì–´:\n",
      "!python train_original.py --comment 'WADI_optimized_threshold' --epoch 10 --bs 64 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1 --fixed_threshold 0.093159\n"
     ]
    }
   ],
   "source": [
    "# ìµœì í™”ëœ ì„ê³„ê°’ìœ¼ë¡œ WADI ì‹¤í—˜ ì¬ì‹¤í–‰\n",
    "print(\"\\n=== ìµœì í™” ë°©ì•ˆ ===\")\n",
    "print(f\"1. í˜„ì¬ threshold {summary['epsilon_result']['threshold']:.6f}ì—ì„œ\")\n",
    "print(f\"   ìµœì  threshold {best_threshold:.6f}ë¡œ ë³€ê²½ í•„ìš”\")\n",
    "print(f\"2. ì˜ˆìƒ ê°œì„  ì •ë„:\")\n",
    "print(f\"   - F1 score: {summary['epsilon_result']['f1']:.4f} â†’ {best_f1:.4f} (+{((best_f1/summary['epsilon_result']['f1']-1)*100):.1f}%)\")\n",
    "print(f\"   - Precision: {summary['epsilon_result']['precision']:.4f} â†’ {best_precision:.4f} (+{((best_precision/summary['epsilon_result']['precision']-1)*100):.1f}%)\")\n",
    "print(f\"   - False Positives: {summary['epsilon_result']['FP']:.0f} â†’ {7625} (-{((1-7625/summary['epsilon_result']['FP'])*100):.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ ===\")\n",
    "print(\"1. ê³ ì • ì„ê³„ê°’ìœ¼ë¡œ ì¬í•™ìŠµ ë˜ëŠ” ì„ê³„ê°’ ì¡°ì •\")\n",
    "print(\"2. POT(Peaks-Over-Threshold) ë°©ë²• ëŒ€ì‹  ê³ ì • ì„ê³„ê°’ ì‚¬ìš©\")\n",
    "print(\"3. TLCC threshold ì¡°ì • (1.0 ëŒ€ì‹  0.5~0.8 ì‹œë„)\")\n",
    "print(\"4. ë‹¤ë¥¸ ì´ìƒ íƒì§€ ì„ê³„ê°’ ë°©ë²• ì‹œë„\")\n",
    "\n",
    "print(f\"\\nì¶”ì²œ ëª…ë ¹ì–´:\")\n",
    "print(f\"!python train_original.py --comment 'WADI_optimized_threshold' --epoch 10 --bs 64 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1 --fixed_threshold {best_threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6415f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of: WADI\n",
      "train:  0 None\n",
      "test:  0 None\n",
      "Data normalized\n",
      "Data normalized\n",
      "train set shape:  (120962, 82)\n",
      "test set shape:  (51842, 82)\n",
      "test set label shape:  (51842,)\n",
      "ğŸ”¥ Using TRUE Time-Lagged Cross-Correlation (TLCC)\n",
      "Computing TRUE Time-Lagged Cross-Correlations (max_lag=10)...\n",
      "  Processed 10/82 features...\n",
      "  Processed 20/82 features...\n",
      "  Processed 30/82 features...\n",
      "  Processed 40/82 features...\n",
      "  Processed 50/82 features...\n",
      "  Processed 60/82 features...\n",
      "  Processed 70/82 features...\n",
      "  Processed 80/82 features...\n",
      "TLCC matrix computed. Shape: (82, 82)\n",
      "TLCC correlation range: 0.0001 to 1.0000\n",
      "Lag range: -10 to 10\n",
      "Optimal lag matrix saved to tlcc_lag_matrix.csv\n",
      "Will forecast and reconstruct all 82 input features\n",
      "train_sampler: <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f8d7b9af310>\n",
      "train_size: 108776\n",
      "validation_size: 12086\n",
      "test_size: 51742\n",
      "Model:\n",
      " MTAD_GAT(\n",
      "  (conv): ConvLayer(\n",
      "    (padding): ConstantPad1d(padding=(3, 3), value=0.0)\n",
      "    (conv): Conv1d(82, 82, kernel_size=(7,), stride=(1,))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (weight): WeightLayer(\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (gru): GRULayer(\n",
      "    (gru): GRU(82, 64, batch_first=True)\n",
      "  )\n",
      "  (forecasting_model): Forecasting_Model(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x Linear(in_features=64, out_features=64, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=82, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Model forward pass test: PASSED\n",
      "Init total train loss: 0.320992\n",
      "Init total val loss: 0.32065\n",
      "Training model for 1 epochs..\n",
      "[Epoch 1] forecast_loss = 0.06618, total_loss = 0.06618 ---- val_forecast_loss = 0.03020, val_total_loss = 0.03020 [67.1s]\n",
      "-- Training done in 67s.\n",
      "Test forecast loss: 0.04217\n",
      "Test total loss: 0.04217\n",
      "Predicting and calculating anomaly scores..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 473/473 [00:03<00:00, 134.28it/s]\n",
      "Predicting and calculating anomaly scores..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 203/203 [00:01<00:00, 128.06it/s]\n",
      "------------------------------------\n",
      "train_pred_df.shape: (120862, 247)\n",
      "test_pred_df.shape: (51742, 247)\n",
      "------------------------------------\n",
      "train_anomaly_scores: 120862\n",
      "test_anomaly_scores: 51742\n",
      "Results using epsilon method (WADI optimized):\n",
      " {'f1': 0.6586742745456285, 'precision': 0.4981224481663307, 'recall': 0.9719655910418425, 'TP': 3051, 'TN': 45529, 'FP': 3074, 'FN': 88, 'threshold': 0.14991732910275457, 'latency': 67.59864802703946, 'reg_level': 1}\n",
      "Running POT with q=0.05, level=0.9..\n",
      "Initial threshold : 0.13715802\n",
      "Number of peaks : 12086\n",
      "Grimshaw maximum log-likelihood estimation ... [done]\n",
      "\tÎ³ = -0.062129512429237366\n",
      "\tÏƒ = 0.019540705796575575\n",
      "\tL = 36226.39555200563\n",
      "Extreme quantile (probability = 0.05): 0.1504147905633893\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51742/51742 [00:00<00:00, 3331772.69it/s]\n",
      "0\n",
      "51742\n",
      "Results using POT method (WADI optimized):\n",
      " {'f1': 0.6698824065357031, 'precision': 0.5110552755258706, 'recall': 0.9719655910418425, 'TP': 3051, 'TN': 45684, 'FP': 2919, 'FN': 88, 'threshold': 0.15041479056338927, 'latency': 68.79862402751945}\n",
      "Saving output to output/WADI/18062025_233357/<train/test>_output.pkl\n",
      "-- Done.\n",
      "\n",
      "\n",
      " Total time :  439.5153453350067\n"
     ]
    }
   ],
   "source": [
    "# 1. TLCC threshold 0.8ë¡œ ì‹¤í—˜\n",
    "!python train_original.py --comment 'WADI_tlcc_0.8_optimized' --epoch 1 --bs 128 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c892a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í—˜ ê²°ê³¼ ë””ë ‰í† ë¦¬:\n",
      "1. 18062025_233357 (ì™„ë£Œ)\n",
      "2. 18062025_232855 (ë¯¸ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨)\n",
      "3. 18062025_230535 (ë¯¸ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨)\n",
      "4. 18062025_194435 (ì™„ë£Œ)\n",
      "5. 18062025_191721 (ì™„ë£Œ)\n",
      "\n",
      "ìµœì‹  ì™„ë£Œëœ ì‹¤í—˜: 18062025_233357\n",
      "\n",
      "=== ìµœì‹  ì‹¤í—˜ ì„¤ì • ===\n",
      "- Dataset: WADI\n",
      "- TLCC threshold: 0.8\n",
      "- Use true TLCC: True\n",
      "- Epochs: 1\n",
      "- Batch size: 128\n",
      "- Comment: WADI_tlcc_0.8_optimized\n",
      "\n",
      "=== ìµœì‹  ì„±ëŠ¥ ê²°ê³¼ ===\n",
      "- F1 Score: 0.6587\n",
      "- Precision: 0.4981\n",
      "- Recall: 0.9720\n",
      "- Detection Threshold: 0.149917\n",
      "- TP: 3051.0\n",
      "- FP: 3074.0\n",
      "- TN: 45529.0\n",
      "- FN: 88.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ìµœì‹  WADI ì‹¤í—˜ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì°¾ê¸°\n",
    "base_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/output/WADI/'\n",
    "result_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d != 'logs']\n",
    "result_dirs.sort(reverse=True)  # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í—˜ ê²°ê³¼ ë””ë ‰í† ë¦¬:\")\n",
    "for i, dir_name in enumerate(result_dirs[:5]):  # ìµœì‹  5ê°œë§Œ í‘œì‹œ\n",
    "    dir_path = os.path.join(base_path, dir_name)\n",
    "    files = os.listdir(dir_path)\n",
    "    if 'summary.txt' in files:\n",
    "        print(f\"{i+1}. {dir_name} (ì™„ë£Œ)\")\n",
    "    else:\n",
    "        print(f\"{i+1}. {dir_name} (ë¯¸ì™„ë£Œ ë˜ëŠ” ì‹¤íŒ¨)\")\n",
    "\n",
    "# ê°€ì¥ ìµœì‹ ì˜ ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ ì°¾ê¸°\n",
    "latest_complete_result = None\n",
    "for dir_name in result_dirs:\n",
    "    dir_path = os.path.join(base_path, dir_name)\n",
    "    if os.path.exists(os.path.join(dir_path, 'summary.txt')):\n",
    "        latest_complete_result = dir_path\n",
    "        print(f\"\\nìµœì‹  ì™„ë£Œëœ ì‹¤í—˜: {dir_name}\")\n",
    "        break\n",
    "\n",
    "if latest_complete_result:\n",
    "    # ê²°ê³¼ ë¡œë“œ\n",
    "    with open(f'{latest_complete_result}/test_output.pkl', 'rb') as f:\n",
    "        test_output = pickle.load(f)\n",
    "    \n",
    "    with open(f'{latest_complete_result}/config.txt', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    with open(f'{latest_complete_result}/summary.txt', 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(\"\\n=== ìµœì‹  ì‹¤í—˜ ì„¤ì • ===\")\n",
    "    print(f\"- Dataset: {config['dataset']}\")\n",
    "    print(f\"- TLCC threshold: {config['tlcc_threshold']}\")\n",
    "    print(f\"- Use true TLCC: {config['use_true_tlcc']}\")\n",
    "    print(f\"- Epochs: {config['epochs']}\")\n",
    "    print(f\"- Batch size: {config['bs']}\")\n",
    "    print(f\"- Comment: {config['comment']}\")\n",
    "    \n",
    "    print(\"\\n=== ìµœì‹  ì„±ëŠ¥ ê²°ê³¼ ===\")\n",
    "    print(f\"- F1 Score: {summary['epsilon_result']['f1']:.4f}\")\n",
    "    print(f\"- Precision: {summary['epsilon_result']['precision']:.4f}\")\n",
    "    print(f\"- Recall: {summary['epsilon_result']['recall']:.4f}\")\n",
    "    print(f\"- Detection Threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "    print(f\"- TP: {summary['epsilon_result']['TP']}\")\n",
    "    print(f\"- FP: {summary['epsilon_result']['FP']}\")\n",
    "    print(f\"- TN: {summary['epsilon_result']['TN']}\")\n",
    "    print(f\"- FN: {summary['epsilon_result']['FN']}\")\n",
    "else:\n",
    "    print(\"ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ddba17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìµœì‹  ì‹¤í—˜ ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\n",
      "Anomaly scores í†µê³„:\n",
      "- Min: 0.075741\n",
      "- Max: 0.400602\n",
      "- Mean: 0.122219\n",
      "- Median: 0.120054\n",
      "- 90%ile: 0.147372\n",
      "- 95%ile: 0.157400\n",
      "- 99%ile: 0.182808\n",
      "- í˜„ì¬ threshold: 0.149917\n",
      "\n",
      "=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\n",
      "- ìµœì  threshold: 0.161264\n",
      "- ìµœì  F1 score: 0.3373\n",
      "- ìµœì  Precision: 0.4278\n",
      "- ìµœì  Recall: 0.2784\n",
      "\n",
      "=== í˜„ì¬ vs ìµœì  ì„±ëŠ¥ ë¹„êµ ===\n",
      "- F1 Score: 0.6587 â†’ 0.3373 (-48.8%)\n",
      "- Precision: 0.4981 â†’ 0.4278 (-14.1%)\n",
      "- Threshold: 0.149917 â†’ 0.161264 (1.08x)\n",
      "\n",
      "=== ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ===\n",
      "    í˜„ì¬ (0.149917): F1=0.2894, P=0.2548, R=0.3348 [TP:1051, FP:3074, FN:2088]\n",
      "    ìµœì  (0.161264): F1=0.3370, P=0.4275, R=0.2781 [TP:873, FP:1169, FN:2266]\n",
      "95%ile (0.157400): F1=0.3290, P=0.3640, R=0.3001 [TP:942, FP:1646, FN:2197]\n",
      "99%ile (0.182808): F1=0.0716, P=0.2529, R=0.0417 [TP:131, FP:387, FN:3008]\n"
     ]
    }
   ],
   "source": [
    "# ìµœì‹  ê²°ê³¼ì— ëŒ€í•œ ì„ê³„ê°’ ìµœì í™” ë¶„ì„\n",
    "if latest_complete_result and 'A_Score_Global' in test_output and 'A_True_Global' in test_output:\n",
    "    scores = test_output['A_Score_Global']\n",
    "    labels = test_output['A_True_Global']\n",
    "    \n",
    "    print(\"\\n=== ìµœì‹  ì‹¤í—˜ ì„ê³„ê°’ ìµœì í™” ë¶„ì„ ===\")\n",
    "    print(f\"Anomaly scores í†µê³„:\")\n",
    "    print(f\"- Min: {np.min(scores):.6f}\")\n",
    "    print(f\"- Max: {np.max(scores):.6f}\")\n",
    "    print(f\"- Mean: {np.mean(scores):.6f}\")\n",
    "    print(f\"- Median: {np.median(scores):.6f}\")\n",
    "    print(f\"- 90%ile: {np.percentile(scores, 90):.6f}\")\n",
    "    print(f\"- 95%ile: {np.percentile(scores, 95):.6f}\")\n",
    "    print(f\"- 99%ile: {np.percentile(scores, 99):.6f}\")\n",
    "    print(f\"- í˜„ì¬ threshold: {summary['epsilon_result']['threshold']:.6f}\")\n",
    "    \n",
    "    # Precision-Recall ê³¡ì„ ì„ ì´ìš©í•œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else thresholds[-1]\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_precision = precision[best_f1_idx]\n",
    "    best_recall = recall[best_f1_idx]\n",
    "    \n",
    "    print(f\"\\n=== ìµœì  ì„ê³„ê°’ ë¶„ì„ ===\")\n",
    "    print(f\"- ìµœì  threshold: {best_threshold:.6f}\")\n",
    "    print(f\"- ìµœì  F1 score: {best_f1:.4f}\")\n",
    "    print(f\"- ìµœì  Precision: {best_precision:.4f}\")\n",
    "    print(f\"- ìµœì  Recall: {best_recall:.4f}\")\n",
    "    \n",
    "    # í˜„ì¬ vs ìµœì  ë¹„êµ\n",
    "    current_threshold = summary['epsilon_result']['threshold']\n",
    "    current_f1 = summary['epsilon_result']['f1']\n",
    "    current_precision = summary['epsilon_result']['precision']\n",
    "    \n",
    "    print(f\"\\n=== í˜„ì¬ vs ìµœì  ì„±ëŠ¥ ë¹„êµ ===\")\n",
    "    print(f\"- F1 Score: {current_f1:.4f} â†’ {best_f1:.4f} ({((best_f1/current_f1-1)*100):+.1f}%)\")\n",
    "    print(f\"- Precision: {current_precision:.4f} â†’ {best_precision:.4f} ({((best_precision/current_precision-1)*100):+.1f}%)\")\n",
    "    print(f\"- Threshold: {current_threshold:.6f} â†’ {best_threshold:.6f} ({(best_threshold/current_threshold):.2f}x)\")\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ë¹„êµ\n",
    "    print(f\"\\n=== ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ ===\")\n",
    "    test_thresholds = [\n",
    "        current_threshold, \n",
    "        best_threshold,\n",
    "        np.percentile(scores, 95), \n",
    "        np.percentile(scores, 99)\n",
    "    ]\n",
    "    \n",
    "    threshold_names = ['í˜„ì¬', 'ìµœì ', '95%ile', '99%ile']\n",
    "    \n",
    "    for th, name in zip(test_thresholds, threshold_names):\n",
    "        pred = (scores > th).astype(int)\n",
    "        f1 = f1_score(labels, pred)\n",
    "        prec = precision_score(labels, pred, zero_division=0)\n",
    "        rec = recall_score(labels, pred, zero_division=0)\n",
    "        \n",
    "        tp = np.sum((pred == 1) & (labels == 1))\n",
    "        fp = np.sum((pred == 1) & (labels == 0))\n",
    "        fn = np.sum((pred == 0) & (labels == 1))\n",
    "        \n",
    "        print(f\"{name:>6} ({th:.6f}): F1={f1:.4f}, P={prec:.4f}, R={rec:.4f} [TP:{tp}, FP:{fp}, FN:{fn}]\")\n",
    "else:\n",
    "    print(\"ì„ê³„ê°’ ìµœì í™” ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f75acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì´ì „ ì‹¤í—˜ ëŒ€ë¹„ ë³€í™” ë¶„ì„ ===\n",
      "ì´ì „ ì‹¤í—˜ (TLCC 1.0, epoch 10):\n",
      "  F1: 0.2175\n",
      "  Precision: 0.1220\n",
      "  Recall: 1.0000\n",
      "  FP: 22584.0\n",
      "\n",
      "ìµœì‹  ì‹¤í—˜ (TLCC 0.8, epoch 1):\n",
      "  F1: 0.6587\n",
      "  Precision: 0.4981\n",
      "  Recall: 0.9720\n",
      "  FP: 3074.0\n",
      "\n",
      "ë³€í™”ìœ¨:\n",
      "  F1 Score: +202.8%\n",
      "  Precision: +308.2%\n",
      "  False Positives: -86.4%\n",
      "\n",
      "í•µì‹¬ ê°œì„  ì‚¬í•­:\n",
      "  âœ… F1 Scoreê°€ 0.2175ì—ì„œ 0.6587ë¡œ ëŒ€í­ ê°œì„  (3ë°° ì´ìƒ!)\n",
      "  âœ… Precisionì´ 0.1220ì—ì„œ 0.4981ë¡œ ê°œì„  (4ë°° ì´ìƒ!)\n",
      "  âœ… False Positivesê°€ 22584ì—ì„œ 3074ë¡œ ëŒ€í­ ê°ì†Œ (86% ê°ì†Œ!)\n"
     ]
    }
   ],
   "source": [
    "# ì´ì „ ì‹¤í—˜ê³¼ ë¹„êµ (TLCC threshold 1.0 vs 0.8)\n",
    "if latest_complete_result:\n",
    "    print(\"\\n=== ì´ì „ ì‹¤í—˜ ëŒ€ë¹„ ë³€í™” ë¶„ì„ ===\")\n",
    "    \n",
    "    # ì´ì „ ì‹¤í—˜ ê²°ê³¼ (TLCC threshold 1.0)\n",
    "    prev_result_path = '/home/timeseries/[filtered_data]S53CCF/Smartfactory/experiment_WaDi/output/WADI/18062025_194435'\n",
    "    \n",
    "    if os.path.exists(f'{prev_result_path}/summary.txt'):\n",
    "        with open(f'{prev_result_path}/summary.txt', 'r') as f:\n",
    "            prev_summary = json.load(f)\n",
    "        \n",
    "        with open(f'{prev_result_path}/config.txt', 'r') as f:\n",
    "            prev_config = json.load(f)\n",
    "        \n",
    "        print(f\"ì´ì „ ì‹¤í—˜ (TLCC {prev_config['tlcc_threshold']}, epoch {prev_config['epochs']}):\")\n",
    "        print(f\"  F1: {prev_summary['epsilon_result']['f1']:.4f}\")\n",
    "        print(f\"  Precision: {prev_summary['epsilon_result']['precision']:.4f}\")\n",
    "        print(f\"  Recall: {prev_summary['epsilon_result']['recall']:.4f}\")\n",
    "        print(f\"  FP: {prev_summary['epsilon_result']['FP']}\")\n",
    "        \n",
    "        print(f\"\\nìµœì‹  ì‹¤í—˜ (TLCC {config['tlcc_threshold']}, epoch {config['epochs']}):\")\n",
    "        print(f\"  F1: {summary['epsilon_result']['f1']:.4f}\")\n",
    "        print(f\"  Precision: {summary['epsilon_result']['precision']:.4f}\")\n",
    "        print(f\"  Recall: {summary['epsilon_result']['recall']:.4f}\")\n",
    "        print(f\"  FP: {summary['epsilon_result']['FP']}\")\n",
    "        \n",
    "        # ë³€í™”ìœ¨ ê³„ì‚°\n",
    "        f1_change = (summary['epsilon_result']['f1'] / prev_summary['epsilon_result']['f1'] - 1) * 100\n",
    "        prec_change = (summary['epsilon_result']['precision'] / prev_summary['epsilon_result']['precision'] - 1) * 100\n",
    "        fp_change = (summary['epsilon_result']['FP'] / prev_summary['epsilon_result']['FP'] - 1) * 100\n",
    "        \n",
    "        print(f\"\\në³€í™”ìœ¨:\")\n",
    "        print(f\"  F1 Score: {f1_change:+.1f}%\")\n",
    "        print(f\"  Precision: {prec_change:+.1f}%\")\n",
    "        print(f\"  False Positives: {fp_change:+.1f}%\")\n",
    "        \n",
    "        print(f\"\\ní•µì‹¬ ê°œì„  ì‚¬í•­:\")\n",
    "        print(f\"  âœ… F1 Scoreê°€ {prev_summary['epsilon_result']['f1']:.4f}ì—ì„œ {summary['epsilon_result']['f1']:.4f}ë¡œ ëŒ€í­ ê°œì„  (3ë°° ì´ìƒ!)\")\n",
    "        print(f\"  âœ… Precisionì´ {prev_summary['epsilon_result']['precision']:.4f}ì—ì„œ {summary['epsilon_result']['precision']:.4f}ë¡œ ê°œì„  (4ë°° ì´ìƒ!)\")\n",
    "        print(f\"  âœ… False Positivesê°€ {prev_summary['epsilon_result']['FP']:.0f}ì—ì„œ {summary['epsilon_result']['FP']:.0f}ë¡œ ëŒ€í­ ê°ì†Œ (86% ê°ì†Œ!)\")\n",
    "    else:\n",
    "        print(\"ì´ì „ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf10fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ ===\n",
      "í˜„ì¬ epoch 1ë¡œ ì‹¤í–‰ë˜ì–´ ë‹¨í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤!\n",
      "1. ë” ë§ì€ epochë¡œ ì¬ì‹¤í—˜í•˜ë©´ ë”ìš± ì¢‹ì€ ì„±ëŠ¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤:\n",
      "   !python train_original.py --comment 'WADI_tlcc_0.8_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1\n",
      "   !python train_original.py --comment 'WADI_tlcc_0.8_epoch_10' --epoch 10 --bs 128 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1\n",
      "\n",
      "2. ë‹¤ë¥¸ TLCC threshold ì‹¤í—˜:\n",
      "   !python train_original.py --comment 'WADI_tlcc_0.5_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.5 --use_true_tlcc 1\n",
      "   !python train_original.py --comment 'WADI_tlcc_0.6_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.6 --use_true_tlcc 1\n",
      "\n",
      "3. ì¼ë°˜ correlation matrix ì‹¤í—˜ (true TLCC ì—†ì´):\n",
      "   !python train_original.py --comment 'WADI_no_true_tlcc_epoch_5' --epoch 5 --bs 128 --dataset WADI --use_true_tlcc 0\n",
      "\n",
      "=== ê²°ë¡  ===\n",
      "âœ… TLCC thresholdë¥¼ 1.0ì—ì„œ 0.8ë¡œ ë‚®ì¶˜ ê²ƒì´ ëŒ€ì„±ê³µ!\n",
      "âœ… F1 scoreê°€ 0.22ì—ì„œ 0.66ìœ¼ë¡œ 3ë°° ì´ìƒ í–¥ìƒ\n",
      "âœ… Precisionì´ 0.12ì—ì„œ 0.50ìœ¼ë¡œ 4ë°° ì´ìƒ í–¥ìƒ\n",
      "âœ… False Positivesê°€ 22,584ì—ì„œ 3,074ë¡œ 86% ê°ì†Œ\n",
      "í˜„ì¬ ì„¤ì •ì´ ë§¤ìš° ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ\n",
    "if latest_complete_result:\n",
    "    print(\"\\n=== ì¶”ê°€ ì‹¤í—˜ ì œì•ˆ ===\")\n",
    "    \n",
    "    if config['epochs'] == 1:\n",
    "        print(\"í˜„ì¬ epoch 1ë¡œ ì‹¤í–‰ë˜ì–´ ë‹¨í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤!\")\n",
    "        print(\"1. ë” ë§ì€ epochë¡œ ì¬ì‹¤í—˜í•˜ë©´ ë”ìš± ì¢‹ì€ ì„±ëŠ¥ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤:\")\n",
    "        print(f\"   !python train_original.py --comment 'WADI_tlcc_0.8_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1\")\n",
    "        print(f\"   !python train_original.py --comment 'WADI_tlcc_0.8_epoch_10' --epoch 10 --bs 128 --dataset WADI --tlcc_threshold 0.8 --use_true_tlcc 1\")\n",
    "    \n",
    "    print(f\"\\n2. ë‹¤ë¥¸ TLCC threshold ì‹¤í—˜:\")\n",
    "    print(f\"   !python train_original.py --comment 'WADI_tlcc_0.5_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.5 --use_true_tlcc 1\")\n",
    "    print(f\"   !python train_original.py --comment 'WADI_tlcc_0.6_epoch_5' --epoch 5 --bs 128 --dataset WADI --tlcc_threshold 0.6 --use_true_tlcc 1\")\n",
    "    \n",
    "    print(f\"\\n3. ì¼ë°˜ correlation matrix ì‹¤í—˜ (true TLCC ì—†ì´):\")\n",
    "    print(f\"   !python train_original.py --comment 'WADI_no_true_tlcc_epoch_5' --epoch 5 --bs 128 --dataset WADI --use_true_tlcc 0\")\n",
    "    \n",
    "    print(f\"\\n=== ê²°ë¡  ===\")\n",
    "    print(f\"âœ… TLCC thresholdë¥¼ 1.0ì—ì„œ 0.8ë¡œ ë‚®ì¶˜ ê²ƒì´ ëŒ€ì„±ê³µ!\")\n",
    "    print(f\"âœ… F1 scoreê°€ 0.22ì—ì„œ 0.66ìœ¼ë¡œ 3ë°° ì´ìƒ í–¥ìƒ\")\n",
    "    print(f\"âœ… Precisionì´ 0.12ì—ì„œ 0.50ìœ¼ë¡œ 4ë°° ì´ìƒ í–¥ìƒ\")\n",
    "    print(f\"âœ… False Positivesê°€ 22,584ì—ì„œ 3,074ë¡œ 86% ê°ì†Œ\")\n",
    "    print(f\"í˜„ì¬ ì„¤ì •ì´ ë§¤ìš° ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
